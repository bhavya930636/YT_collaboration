{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cd605e2-9f53-4fe5-862e-0e875f3f51fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/bhoomi/miniconda3/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bhoomi/miniconda3/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bhoomi/miniconda3/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bhoomi/miniconda3/lib/python3.12/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bhoomi/miniconda3/lib/python3.12/site-packages (from requests) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e06702-00d9-416b-b897-54c348d5f969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected video IDs: ['khyg5YJlLPI', 'qP8HTuEu7BY', 'On4GE5hAU8s']\n",
      "\n",
      "Video Upload Timestamps:\n",
      "khyg5YJlLPI: 2017-01-11T14:00:04Z\n",
      "qP8HTuEu7BY: 2017-01-09T22:00:01Z\n",
      "On4GE5hAU8s: 2017-02-11T17:10:18Z\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import requests\n",
    "\n",
    "# Sample dictionary\n",
    "data = {\n",
    "    \"cluster\": [220, 11097, 5556],\n",
    "    \"from\": [\"UCq6VFHwMzcMXbuKyG7SQYIg\", \"UCq6VFHwMzcMXbuKyG7SQYIg\", \"UClpEE-Led9ZK0GJQKvU--3Q\"],\n",
    "    \"videos\": ['[\"khyg5YJlLPI\"]', '[\"qP8HTuEu7BY\"]', '[\"On4GE5hAU8s\"]']\n",
    "}\n",
    "\n",
    "# Extract video IDs from the dictionary\n",
    "video_ids = []\n",
    "\n",
    "for v in data[\"videos\"]:\n",
    "    try:\n",
    "        ids = ast.literal_eval(v)\n",
    "        video_ids.extend(ids)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping due to error: {e}\")\n",
    "\n",
    "print(\"Collected video IDs:\", video_ids)\n",
    "\n",
    "# Query YouTube API\n",
    "API_KEY = 'AIzaSyDO9W2xxpc7ud4W8N9L06n2Mwv5QkMtoFc'  # Replace with your actual key\n",
    "url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "\n",
    "def chunkify(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "timestamps = {}\n",
    "\n",
    "for chunk in chunkify(video_ids, 50):\n",
    "    ids_str = \",\".join(chunk)\n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"id\": ids_str,\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    for item in data.get(\"items\", []):\n",
    "        video_id = item[\"id\"]\n",
    "        published_at = item[\"snippet\"][\"publishedAt\"]\n",
    "        timestamps[video_id] = published_at\n",
    "\n",
    "print(\"\\nVideo Upload Timestamps:\")\n",
    "for vid, ts in timestamps.items():\n",
    "    print(f\"{vid}: {ts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "103b9bec-7ef3-4294-82d3-1f91f5f3bcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected video IDs: ['khyg5YJlLPI', 'qP8HTuEu7BY', 'On4GE5hAU8s']\n",
      "\n",
      "Video Upload Info with Captions:\n",
      "\n",
      "Video ID: khyg5YJlLPI\n",
      "  Uploaded at: 2017-01-11T14:00:04Z\n",
      "  Caption Preview: what if Bruce Wayne didn't survive in the alley what if his father did who would become\n",
      "\n",
      "Video ID: qP8HTuEu7BY\n",
      "  Uploaded at: 2017-01-09T22:00:01Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: On4GE5hAU8s\n",
      "  Uploaded at: 2017-02-11T17:10:18Z\n",
      "  Caption Preview: Captions disabled or unavailable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import requests\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "\n",
    "# Sample dictionary\n",
    "data = {\n",
    "    \"cluster\": [220, 11097, 5556],\n",
    "    \"from\": [\"UCq6VFHwMzcMXbuKyG7SQYIg\", \"UCq6VFHwMzcMXbuKyG7SQYIg\", \"UClpEE-Led9ZK0GJQKvU--3Q\"],\n",
    "    \"videos\": ['[\"khyg5YJlLPI\"]', '[\"qP8HTuEu7BY\"]', '[\"On4GE5hAU8s\"]']\n",
    "}\n",
    "\n",
    "# Extract video IDs\n",
    "video_ids = []\n",
    "for v in data[\"videos\"]:\n",
    "    try:\n",
    "        ids = ast.literal_eval(v)\n",
    "        video_ids.extend(ids)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping due to error: {e}\")\n",
    "\n",
    "print(\"Collected video IDs:\", video_ids)\n",
    "\n",
    "# YouTube API config\n",
    "API_KEY = 'AIzaSyDO9W2xxpc7ud4W8N9L06n2Mwv5QkMtoFc'  # Replace with your real API key\n",
    "url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "\n",
    "def chunkify(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "timestamps = {}\n",
    "captions = {}\n",
    "\n",
    "for chunk in chunkify(video_ids, 50):\n",
    "    ids_str = \",\".join(chunk)\n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"id\": ids_str,\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    for item in data.get(\"items\", []):\n",
    "        video_id = item[\"id\"]\n",
    "        published_at = item[\"snippet\"][\"publishedAt\"]\n",
    "        timestamps[video_id] = published_at\n",
    "\n",
    "        # Fetch captions\n",
    "        try:\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
    "            preview = \" \".join([line['text'] for line in transcript[:3]])\n",
    "            captions[video_id] = preview\n",
    "        except TranscriptsDisabled:\n",
    "            captions[video_id] = \"Captions disabled or unavailable\"\n",
    "        except NoTranscriptFound:\n",
    "            captions[video_id] = \"Captions not available in English\"\n",
    "        except Exception as e:\n",
    "            captions[video_id] = f\"Error fetching captions\"\n",
    "\n",
    "# Final output\n",
    "print(\"\\nVideo Upload Info with Captions:\\n\")\n",
    "for vid in video_ids:\n",
    "    print(f\"Video ID: {vid}\")\n",
    "    print(f\"  Uploaded at: {timestamps.get(vid, 'Unknown')}\")\n",
    "    print(f\"  Caption Preview: {captions.get(vid)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f784b610-89b5-49fd-b3f6-2d471098b0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/bhoomi/miniconda3/lib/python3.12/site-packages (2.32.3)\n",
      "Collecting youtube-transcript-api\n",
      "  Downloading youtube_transcript_api-1.0.3-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bhoomi/miniconda3/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bhoomi/miniconda3/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bhoomi/miniconda3/lib/python3.12/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bhoomi/miniconda3/lib/python3.12/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /home/bhoomi/miniconda3/lib/python3.12/site-packages (from youtube-transcript-api) (0.7.1)\n",
      "Downloading youtube_transcript_api-1.0.3-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: youtube-transcript-api\n",
      "Successfully installed youtube-transcript-api-1.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install requests youtube-transcript-api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aeb13e1-d07d-4916-aded-31a7a00ed553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted data written to: extracted_data.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "input_file = 'data.txt'         # Your input file name\n",
    "output_file = 'extracted_data.csv'  # You can also change to .txt if needed\n",
    "\n",
    "# Open and read TSV input\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8', newline='') as outfile:\n",
    "    reader = csv.DictReader(infile, delimiter='\\t')\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    # Write header\n",
    "    writer.writerow(['cluster', 'from', 'videos'])\n",
    "\n",
    "    for row in reader:\n",
    "        cluster = row['cluster'].strip()\n",
    "        frm = row['from'].strip()\n",
    "        videos = row['videos'].strip()\n",
    "\n",
    "        # Only write valid rows\n",
    "        if cluster and frm and videos:\n",
    "            writer.writerow([cluster, frm, videos])\n",
    "\n",
    "print(f\"‚úÖ Extracted data written to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba74cf8c-5890-40b1-bddd-7465fc8b0a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì° Fetching video metadata and captions...\n",
      "üîπ Processing chunk 1/153...\n",
      "üîπ Processing chunk 2/153...\n",
      "üîπ Processing chunk 3/153...\n",
      "üîπ Processing chunk 4/153...\n",
      "üîπ Processing chunk 5/153...\n",
      "üîπ Processing chunk 6/153...\n",
      "üîπ Processing chunk 7/153...\n",
      "üîπ Processing chunk 8/153...\n",
      "üîπ Processing chunk 9/153...\n",
      "üîπ Processing chunk 10/153...\n",
      "üîπ Processing chunk 11/153...\n",
      "üîπ Processing chunk 12/153...\n",
      "üîπ Processing chunk 13/153...\n",
      "üîπ Processing chunk 14/153...\n",
      "üîπ Processing chunk 15/153...\n",
      "üîπ Processing chunk 16/153...\n",
      "üîπ Processing chunk 17/153...\n",
      "üîπ Processing chunk 18/153...\n",
      "üîπ Processing chunk 19/153...\n",
      "üîπ Processing chunk 20/153...\n",
      "üîπ Processing chunk 21/153...\n",
      "üîπ Processing chunk 22/153...\n",
      "üîπ Processing chunk 23/153...\n",
      "üîπ Processing chunk 24/153...\n",
      "üîπ Processing chunk 25/153...\n",
      "üîπ Processing chunk 26/153...\n",
      "üîπ Processing chunk 27/153...\n",
      "üîπ Processing chunk 28/153...\n",
      "üîπ Processing chunk 29/153...\n",
      "üîπ Processing chunk 30/153...\n",
      "üîπ Processing chunk 31/153...\n",
      "üîπ Processing chunk 32/153...\n",
      "üîπ Processing chunk 33/153...\n",
      "üîπ Processing chunk 34/153...\n",
      "üîπ Processing chunk 35/153...\n",
      "üîπ Processing chunk 36/153...\n",
      "üîπ Processing chunk 37/153...\n",
      "üîπ Processing chunk 38/153...\n",
      "üîπ Processing chunk 39/153...\n",
      "üîπ Processing chunk 40/153...\n",
      "üîπ Processing chunk 41/153...\n",
      "üîπ Processing chunk 42/153...\n",
      "üîπ Processing chunk 43/153...\n",
      "üîπ Processing chunk 44/153...\n",
      "üîπ Processing chunk 45/153...\n",
      "üîπ Processing chunk 46/153...\n",
      "üîπ Processing chunk 47/153...\n",
      "üîπ Processing chunk 48/153...\n",
      "üîπ Processing chunk 49/153...\n",
      "üîπ Processing chunk 50/153...\n",
      "üîπ Processing chunk 51/153...\n",
      "üîπ Processing chunk 52/153...\n",
      "üîπ Processing chunk 53/153...\n",
      "üîπ Processing chunk 54/153...\n",
      "üîπ Processing chunk 55/153...\n",
      "üîπ Processing chunk 56/153...\n",
      "üîπ Processing chunk 57/153...\n",
      "üîπ Processing chunk 58/153...\n",
      "üîπ Processing chunk 59/153...\n",
      "üîπ Processing chunk 60/153...\n",
      "üîπ Processing chunk 61/153...\n",
      "üîπ Processing chunk 62/153...\n",
      "üîπ Processing chunk 63/153...\n",
      "üîπ Processing chunk 64/153...\n",
      "üîπ Processing chunk 65/153...\n",
      "üîπ Processing chunk 66/153...\n",
      "üîπ Processing chunk 67/153...\n",
      "üîπ Processing chunk 68/153...\n",
      "üîπ Processing chunk 69/153...\n",
      "üîπ Processing chunk 70/153...\n",
      "üîπ Processing chunk 71/153...\n",
      "üîπ Processing chunk 72/153...\n",
      "üîπ Processing chunk 73/153...\n",
      "üîπ Processing chunk 74/153...\n",
      "üîπ Processing chunk 75/153...\n",
      "üîπ Processing chunk 76/153...\n",
      "üîπ Processing chunk 77/153...\n",
      "üîπ Processing chunk 78/153...\n",
      "üîπ Processing chunk 79/153...\n",
      "üîπ Processing chunk 80/153...\n",
      "üîπ Processing chunk 81/153...\n",
      "üîπ Processing chunk 82/153...\n",
      "üîπ Processing chunk 83/153...\n",
      "üîπ Processing chunk 84/153...\n",
      "üîπ Processing chunk 85/153...\n",
      "üîπ Processing chunk 86/153...\n",
      "üîπ Processing chunk 87/153...\n",
      "üîπ Processing chunk 88/153...\n",
      "üîπ Processing chunk 89/153...\n",
      "üîπ Processing chunk 90/153...\n",
      "üîπ Processing chunk 91/153...\n",
      "üîπ Processing chunk 92/153...\n",
      "üîπ Processing chunk 93/153...\n",
      "üîπ Processing chunk 94/153...\n",
      "üîπ Processing chunk 95/153...\n",
      "üîπ Processing chunk 96/153...\n",
      "üîπ Processing chunk 97/153...\n",
      "üîπ Processing chunk 98/153...\n",
      "üîπ Processing chunk 99/153...\n",
      "üîπ Processing chunk 100/153...\n",
      "üîπ Processing chunk 101/153...\n",
      "üîπ Processing chunk 102/153...\n",
      "üîπ Processing chunk 103/153...\n",
      "üîπ Processing chunk 104/153...\n",
      "üîπ Processing chunk 105/153...\n",
      "üîπ Processing chunk 106/153...\n",
      "üîπ Processing chunk 107/153...\n",
      "üîπ Processing chunk 108/153...\n",
      "üîπ Processing chunk 109/153...\n",
      "üîπ Processing chunk 110/153...\n",
      "üîπ Processing chunk 111/153...\n",
      "üîπ Processing chunk 112/153...\n",
      "üîπ Processing chunk 113/153...\n",
      "üîπ Processing chunk 114/153...\n",
      "üîπ Processing chunk 115/153...\n",
      "üîπ Processing chunk 116/153...\n",
      "üîπ Processing chunk 117/153...\n",
      "üîπ Processing chunk 118/153...\n",
      "üîπ Processing chunk 119/153...\n",
      "üîπ Processing chunk 120/153...\n",
      "üîπ Processing chunk 121/153...\n",
      "üîπ Processing chunk 122/153...\n",
      "üîπ Processing chunk 123/153...\n",
      "üîπ Processing chunk 124/153...\n",
      "üîπ Processing chunk 125/153...\n",
      "üîπ Processing chunk 126/153...\n",
      "üîπ Processing chunk 127/153...\n",
      "üîπ Processing chunk 128/153...\n",
      "üîπ Processing chunk 129/153...\n",
      "üîπ Processing chunk 130/153...\n",
      "üîπ Processing chunk 131/153...\n",
      "üîπ Processing chunk 132/153...\n",
      "üîπ Processing chunk 133/153...\n",
      "üîπ Processing chunk 134/153...\n",
      "üîπ Processing chunk 135/153...\n",
      "üîπ Processing chunk 136/153...\n",
      "üîπ Processing chunk 137/153...\n",
      "üîπ Processing chunk 138/153...\n",
      "üîπ Processing chunk 139/153...\n",
      "üîπ Processing chunk 140/153...\n",
      "üîπ Processing chunk 141/153...\n",
      "üîπ Processing chunk 142/153...\n",
      "üîπ Processing chunk 143/153...\n",
      "üîπ Processing chunk 144/153...\n",
      "üîπ Processing chunk 145/153...\n",
      "üîπ Processing chunk 146/153...\n",
      "üîπ Processing chunk 147/153...\n",
      "üîπ Processing chunk 148/153...\n",
      "üîπ Processing chunk 149/153...\n",
      "üîπ Processing chunk 150/153...\n",
      "üîπ Processing chunk 151/153...\n",
      "üîπ Processing chunk 152/153...\n",
      "üîπ Processing chunk 153/153...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "import requests\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "\n",
    "# üîÑ Updated file path\n",
    "file_path = 'extracted_data.csv'\n",
    "\n",
    "video_ids = []\n",
    "channel_ids = []\n",
    "cluster_ids = []\n",
    "\n",
    "# üßæ Reading CSV file\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        try:\n",
    "            cluster_ids.append(int(row['cluster']))\n",
    "            channel_ids.append(row['from'])\n",
    "\n",
    "            videos = ast.literal_eval(row['videos'])  # Convert string to list\n",
    "            video_ids.extend(videos)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipping row due to error: {e}\")\n",
    "\n",
    "# üîë YouTube API config\n",
    "API_KEY = 'AIzaSyDO9W2xxpc7ud4W8N9L06n2Mwv5QkMtoFc'\n",
    "url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "\n",
    "def chunkify(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "timestamps = {}\n",
    "captions = {}\n",
    "\n",
    "print(\"üì° Fetching video metadata and captions...\")\n",
    "\n",
    "for chunk_num, chunk in enumerate(chunkify(video_ids, 50)):\n",
    "    print(f\"üîπ Processing chunk {chunk_num + 1}/{(len(video_ids) - 1) // 50 + 1}...\")\n",
    "    ids_str = \",\".join(chunk)\n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"id\": ids_str,\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error fetching video data: {e}\")\n",
    "        continue\n",
    "\n",
    "    for item in data.get(\"items\", []):\n",
    "        video_id = item[\"id\"]\n",
    "        published_at = item[\"snippet\"][\"publishedAt\"]\n",
    "        timestamps[video_id] = published_at\n",
    "\n",
    "        try:\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
    "            preview = \" \".join([line['text'] for line in transcript[:3]])\n",
    "            captions[video_id] = preview\n",
    "        except TranscriptsDisabled:\n",
    "            captions[video_id] = \"Captions disabled or unavailable\"\n",
    "        except NoTranscriptFound:\n",
    "            captions[video_id] = \"Captions not available in English\"\n",
    "        except Exception as e:\n",
    "            captions[video_id] = f\"Error fetching captions: {str(e)}\"\n",
    "\n",
    "# ‚úÖ Final formatted output\n",
    "for vid in video_ids:\n",
    "    print(f\"Video ID: {vid}\")\n",
    "    print(f\"  Uploaded at: {timestamps.get(vid, 'Unknown')}\")\n",
    "    print(f\"  Caption Preview: {captions.get(vid, 'Not Fetched')}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76553b9f-c174-4c24-8c86-5c8743c2a4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì° Fetching video metadata and captions...\n",
      "üîπ Processing chunk 1/2...\n",
      "üîπ Processing chunk 2/2...\n",
      "Video ID: khyg5YJlLPI\n",
      "  Uploaded at: 2017-01-11T14:00:04Z\n",
      "  Caption Preview: what if Bruce Wayne didn't survive in the alley what if his father did who would become\n",
      "\n",
      "Video ID: qP8HTuEu7BY\n",
      "  Uploaded at: 2017-01-09T22:00:01Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: On4GE5hAU8s\n",
      "  Uploaded at: 2017-02-11T17:10:18Z\n",
      "  Caption Preview: Captions disabled or unavailable\n",
      "\n",
      "Video ID: hAvpa3iyrsM\n",
      "  Uploaded at: 2017-01-16T18:00:39Z\n",
      "  Caption Preview: what's up guys it's your boy Wolfie and marrick and we are joined with Nellis and today we are doing a pack opening\n",
      "\n",
      "Video ID: 4cEkXTIcvH4\n",
      "  Uploaded at: 2017-03-25T11:29:30Z\n",
      "  Caption Preview: what wait got did you actually he did he didn't get M did he\n",
      "\n",
      "Video ID: _kOwJy1fOuQ\n",
      "  Uploaded at: 2017-01-14T19:18:51Z\n",
      "  Caption Preview: is it on the player yeah I just got it it's gone who was it Thiago Silva you're lying no n was it really wait this kind\n",
      "\n",
      "Video ID: WvCaYRRL9F8\n",
      "  Uploaded at: 2017-02-12T19:25:05Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: C7Waq5ZgqGU\n",
      "  Uploaded at: Unknown\n",
      "  Caption Preview: Not Fetched\n",
      "\n",
      "Video ID: VNXJbRvu8zE\n",
      "  Uploaded at: 2017-01-24T19:24:03Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: 2LAQMtZKpw8\n",
      "  Uploaded at: 2017-01-20T21:00:00Z\n",
      "  Caption Preview: Jurassic World - The Game Episode 178 Stegoceratops Level 40 Dinosaurs Ludia vs Indominus Gameplay going to be the fourth episode in my new format where I'm running you get through\n",
      "\n",
      "Video ID: KxWE0NJ96eg\n",
      "  Uploaded at: 2017-02-20T21:00:01Z\n",
      "  Caption Preview: Captions disabled or unavailable\n",
      "\n",
      "Video ID: hnbtt8DTrXI\n",
      "  Uploaded at: 2017-01-27T21:00:01Z\n",
      "  Caption Preview: okay guys welcome back today we're going to be hitting level 74 that's going to be awesome getting really near 75 I\n",
      "\n",
      "Video ID: pz8DhJZxbhQ\n",
      "  Uploaded at: 2017-02-27T21:00:01Z\n",
      "  Caption Preview: ok guys we'll come back Jurassic World - The Game Episode 196 New Hybrid Unayrhynchus Dinosaurs Ludia vs Indominus Rex episode drastic world the game we have also battle today we're going to have\n",
      "\n",
      "Video ID: RJir4ghyTDo\n",
      "  Uploaded at: 2017-03-12T06:57:23Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: bWtjQIbp08c\n",
      "  Uploaded at: 2017-03-08T21:30:00Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: Vur2PfNlI_Q\n",
      "  Uploaded at: Unknown\n",
      "  Caption Preview: Not Fetched\n",
      "\n",
      "Video ID: dB50g5a1Dx8\n",
      "  Uploaded at: Unknown\n",
      "  Caption Preview: Not Fetched\n",
      "\n",
      "Video ID: EQtp4c2QBqk\n",
      "  Uploaded at: Unknown\n",
      "  Caption Preview: Not Fetched\n",
      "\n",
      "Video ID: syFKneGDzEQ\n",
      "  Uploaded at: 2017-02-01T14:00:08Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: AVNlxjv7Yvo\n",
      "  Uploaded at: 2017-03-14T22:00:00Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: 9Ewtl2sASU4\n",
      "  Uploaded at: 2017-02-09T02:56:08Z\n",
      "  Caption Preview: [Applause] [Music] [Applause]\n",
      "\n",
      "Video ID: 3WIrb4FvjhI\n",
      "  Uploaded at: 2017-01-29T18:34:13Z\n",
      "  Caption Preview: what's good YouTube it's Aaron here at the house of champions and we special guest today doing the commentary what's\n",
      "\n",
      "Video ID: dRDEBaMa2YA\n",
      "  Uploaded at: 2016-12-29T19:35:22Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: LSj5HI2qesw\n",
      "  Uploaded at: 2017-03-21T20:00:01Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: ECbwCuN-5FE\n",
      "  Uploaded at: 2017-03-03T14:40:57Z\n",
      "  Caption Preview: [Music] ladies and gentlemen welcome to son of a beach tonight's guests are Raa ab and\n",
      "\n",
      "Video ID: 0Fj5uDNGyg8\n",
      "  Uploaded at: 2017-03-03T17:01:00Z\n",
      "  Caption Preview: right guys so today I'm here with Harry Cal and Theo he's always here but we're going to be doing uh different\n",
      "\n",
      "Video ID: 8NGNWUcjmxQ\n",
      "  Uploaded at: 2017-03-12T17:00:05Z\n",
      "  Caption Preview: I'm ready I'm ready [Music] I'm it is back [¬†__¬†] cyon FC\n",
      "\n",
      "Video ID: oF_Rf5t6boA\n",
      "  Uploaded at: Unknown\n",
      "  Caption Preview: Not Fetched\n",
      "\n",
      "Video ID: tdsNqpOLbA0\n",
      "  Uploaded at: 2017-02-25T13:37:07Z\n",
      "  Caption Preview: Captions disabled or unavailable\n",
      "\n",
      "Video ID: 5qfmGkAECFk\n",
      "  Uploaded at: 2017-03-27T17:30:00Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: 5iek4LiMRx8\n",
      "  Uploaded at: 2017-03-01T01:56:19Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: 8g6ndDE0MYA\n",
      "  Uploaded at: 2017-01-28T09:01:02Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: OZXkX368Nsg\n",
      "  Uploaded at: 2017-01-29T12:00:01Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: mze0QYK4GQE\n",
      "  Uploaded at: 2017-01-28T10:00:32Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: rULUEf9faJo\n",
      "  Uploaded at: 2017-01-26T16:00:02Z\n",
      "  Caption Preview: Captions disabled or unavailable\n",
      "\n",
      "Video ID: SgH19fwSjKo\n",
      "  Uploaded at: 2017-02-10T08:34:54Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: 7VQEk6SSAjU\n",
      "  Uploaded at: 2017-02-02T17:36:39Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: aCDN1pAEJaQ\n",
      "  Uploaded at: 2017-03-20T21:00:00Z\n",
      "  Caption Preview: hello and welcome back to most amazing top 10 how are you all doing I'm Rebecca felgate and today's episode of most\n",
      "\n",
      "Video ID: HJv-ZTMd7k8\n",
      "  Uploaded at: 2017-01-21T18:51:46Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: f9fnYaT_X-k\n",
      "  Uploaded at: 2017-03-15T13:30:01Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: t1dE2rNOF9s\n",
      "  Uploaded at: 2017-03-08T13:30:00Z\n",
      "  Caption Preview: Captions disabled or unavailable\n",
      "\n",
      "Video ID: bhdwPrUcO_A\n",
      "  Uploaded at: 2017-02-06T18:30:00Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: 96yuR2Yi3m8\n",
      "  Uploaded at: 2017-03-05T13:30:00Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: 3Z0GQ2Ma8O8\n",
      "  Uploaded at: 2017-03-20T12:32:30Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: 57iu7iXWY04\n",
      "  Uploaded at: 2017-03-07T12:30:17Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: -x3LutVAV14\n",
      "  Uploaded at: Unknown\n",
      "  Caption Preview: Not Fetched\n",
      "\n",
      "Video ID: BGodFlkmmm8\n",
      "  Uploaded at: Unknown\n",
      "  Caption Preview: Not Fetched\n",
      "\n",
      "Video ID: GGNcuhKh6PY\n",
      "  Uploaded at: Unknown\n",
      "  Caption Preview: Not Fetched\n",
      "\n",
      "Video ID: kI2PD4cl2Z4\n",
      "  Uploaded at: Unknown\n",
      "  Caption Preview: Not Fetched\n",
      "\n",
      "Video ID: -QzhouA4x-8\n",
      "  Uploaded at: 2017-01-03T01:17:10Z\n",
      "  Caption Preview: how is it going today loyalist K my name is DNE Wayne Jaz today we have this is a commentary 134 leave it in\n",
      "\n",
      "Video ID: -eaNR0tVZkk\n",
      "  Uploaded at: 2017-02-25T00:00:00Z\n",
      "  Caption Preview: probably one of the hardest guns to use in the game if I do say so definitely not going easy Get Ready Get Ready Get\n",
      "\n",
      "Video ID: 012en7bL0p0\n",
      "  Uploaded at: 2017-01-18T03:28:24Z\n",
      "  Caption Preview: that is awesome what is that nice thank you very much I may have played this game for ages but we're\n",
      "\n",
      "Video ID: 1KpLDC8W5jg\n",
      "  Uploaded at: 2017-03-14T01:00:00Z\n",
      "  Caption Preview: # pray for Alia definitely needs to come into effect me so ladies and gentlemen I hope you're having a fantastic day\n",
      "\n",
      "Video ID: 5J_50xxITno\n",
      "  Uploaded at: 2017-03-06T00:00:00Z\n",
      "  Caption Preview: oh my God this gun is insane like one of the best guns in the game so guys today I have been smashing Call of Duty I\n",
      "\n",
      "Video ID: 6XoC3-aDN4M\n",
      "  Uploaded at: 2017-03-16T01:00:01Z\n",
      "  Caption Preview: U wow UAV wow that was some good range actually great great gun ladies and\n",
      "\n",
      "Video ID: 6nWE6RkeTgI\n",
      "  Uploaded at: 2017-02-07T00:01:42Z\n",
      "  Caption Preview: ladies and gentlemen of almost 30,000 of you guys participating I ran a poll on Twitter asking you which game you want\n",
      "\n",
      "Video ID: 7K5D7j9Eb6Y\n",
      "  Uploaded at: 2017-03-06T23:00:00Z\n",
      "  Caption Preview: I expect almost everyone to be using this weapon if it's the new free DLC gun ladies and gentlemen it it is finally\n",
      "\n",
      "Video ID: 7yFGleDcSEU\n",
      "  Uploaded at: 2017-01-22T01:00:00Z\n",
      "  Caption Preview: number six so about a year ago Alia made a bet with n shot at a Forza tournament that if n shot's video got 100,000 likes\n",
      "\n",
      "Video ID: 8-BrUHxO4UU\n",
      "  Uploaded at: 2017-02-18T00:03:48Z\n",
      "  Caption Preview: C run what's up what's up what's up anyone else we've got our snap wig on oh my go so guys following the trend\n",
      "\n",
      "Video ID: 9QetCNkbfBE\n",
      "  Uploaded at: 2017-03-19T00:00:00Z\n",
      "  Caption Preview: let's go first-ever overwatch game we ever played on find your whiskers I know the ugly side article to the fruits of\n",
      "\n",
      "Video ID: A-FPzmfHee0\n",
      "  Uploaded at: 2017-03-08T22:00:01Z\n",
      "  Caption Preview: [Music] jez I forgot how intense this game got as well freaking awesome so guys I was\n",
      "\n",
      "Video ID: B7X1M94qjrM\n",
      "  Uploaded at: 2017-01-02T00:09:11Z\n",
      "  Caption Preview: so guys if you've been following my Adventures on Black Ops 3 recently you'll know that they added in a few new\n",
      "\n",
      "Video ID: DTya-Jbd6Hg\n",
      "  Uploaded at: 2017-03-12T00:00:02Z\n",
      "  Caption Preview: hello everyone my name is Al and welcome to the video you've all been waiting for I guess welcome everyone this it\n",
      "\n",
      "Video ID: HJ3lMCDyr2o\n",
      "  Uploaded at: 2017-02-19T01:00:00Z\n",
      "  Caption Preview: ladies and gentlemen you probably know what is about to happen we cannot have any more of a throwback Series in this\n",
      "\n",
      "Video ID: HeHC0WlHC_I\n",
      "  Uploaded at: 2017-03-08T00:00:01Z\n",
      "  Caption Preview: so guys the very first modern warfare remaster DLC has just been announced a few of you guys have tweeted at me and I\n",
      "\n",
      "Video ID: IOut7pYgBMA\n",
      "  Uploaded at: 2017-03-09T22:00:01Z\n",
      "  Caption Preview: if I had to go through and give my opinion on which gun I thought was the best which one would I decide welcome\n",
      "\n",
      "Video ID: IwrQFar1B4U\n",
      "  Uploaded at: 2017-01-15T00:05:17Z\n",
      "  Caption Preview: this happens to me all the time and I'm sure you guys can relate especially when you just really should be doing\n",
      "\n",
      "Video ID: KpkQbnCPv_w\n",
      "  Uploaded at: 2017-01-12T22:06:37Z\n",
      "  Caption Preview: so I've just finished reacting to some of my old videos my old cool of Duty videos going down memory lane it was so\n",
      "\n",
      "Video ID: KvwOXyHVLNg\n",
      "  Uploaded at: 2017-01-27T00:05:49Z\n",
      "  Caption Preview: oh God I'm dead I'm dead oh oh my God hey surprise surprise surprise guys\n",
      "\n",
      "Video ID: NBgjMkObPmI\n",
      "  Uploaded at: 2017-02-08T22:18:18Z\n",
      "  Caption Preview: [Music] [Applause] [Music]\n",
      "\n",
      "Video ID: NENvvgcoVRg\n",
      "  Uploaded at: 2017-03-23T00:00:01Z\n",
      "  Caption Preview: [Music] we're going to do it we're going to play all of the maps come on air strike\n",
      "\n",
      "Video ID: OWQz2VjltT0\n",
      "  Uploaded at: 2017-01-19T22:00:02Z\n",
      "  Caption Preview: so I'm sure every single one of you by this point has been click baited into clicking a th000 degree knife video and\n",
      "\n",
      "Video ID: PTGzolusEI8\n",
      "  Uploaded at: 2017-02-07T23:07:41Z\n",
      "  Caption Preview: [Music] w [Music]\n",
      "\n",
      "Video ID: QaDgeSr2RrA\n",
      "  Uploaded at: 2017-03-11T01:00:02Z\n",
      "  Caption Preview: ladies and gentlemen today is a monumentous day a day that I am very excited for because believe it or not\n",
      "\n",
      "Video ID: SkSmtpmkXKU\n",
      "  Uploaded at: 2017-02-05T00:21:55Z\n",
      "  Caption Preview: we have to get at least one kill with these things you know what I mean yeah two will do two will do dude you know\n",
      "\n",
      "Video ID: SvSNshPNQnI\n",
      "  Uploaded at: 2017-02-15T01:00:01Z\n",
      "  Caption Preview: [Music] [Music] even at close range this thing\n",
      "\n",
      "Video ID: T7XFQ6KSMkk\n",
      "  Uploaded at: 2017-02-23T00:00:02Z\n",
      "  Caption Preview: they be infected they have like a million more Health jeez so many infected coming at me from\n",
      "\n",
      "Video ID: UdZSGd35qQY\n",
      "  Uploaded at: 2017-02-12T01:00:01Z\n",
      "  Caption Preview: it looks like the M16 but how powerful is this thing let's have a look oh my this could be potentially ridiculous\n",
      "\n",
      "Video ID: W1wI48Ok2v4\n",
      "  Uploaded at: 2017-03-19T22:00:02Z\n",
      "  Caption Preview: so guys it took me a little bit of hunting but I have found it Advanced Warfare my friends is the first edition\n",
      "\n",
      "Video ID: XmUvlHTX-9w\n",
      "  Uploaded at: 2017-03-17T02:00:02Z\n",
      "  Caption Preview: [Music] Che so out of nowhere the Black Ops 3 gods have dropped us a brand new set of\n",
      "\n",
      "Video ID: Z7Qzke8R1g8\n",
      "  Uploaded at: 2017-03-26T01:00:03Z\n",
      "  Caption Preview: oh you ain't safe F oo jeez so guys cod's a little bit\n",
      "\n",
      "Video ID: e4IzWgWp9cQ\n",
      "  Uploaded at: 2017-03-04T00:00:02Z\n",
      "  Caption Preview: oh what welcome everyone I hope you're having an amazing amazing day CLA and I have actually just returned from La\n",
      "\n",
      "Video ID: f27O3MNjYsA\n",
      "  Uploaded at: 2017-03-25T02:25:49Z\n",
      "  Caption Preview: call of duty 2017 the brand-new code title is look set to be cooled cool of duty World War two around this time\n",
      "\n",
      "Video ID: iNxPdm8lqDE\n",
      "  Uploaded at: 2017-02-10T22:00:02Z\n",
      "  Caption Preview: Call of Duty 2017 what is going to be happening are we going to be going back into space are\n",
      "\n",
      "Video ID: jzorGG5LfSw\n",
      "  Uploaded at: 2017-01-25T00:00:02Z\n",
      "  Caption Preview: aliia what do you honestly think and why do you honestly not play Infinite Warfare It's a question I see all the\n",
      "\n",
      "Video ID: k2S5I7DVnF0\n",
      "  Uploaded at: 2017-01-05T01:27:30Z\n",
      "  Caption Preview: so I think whenever I meet people for the first time the most common thing they ever say is wow you're a lot taller\n",
      "\n",
      "Video ID: kamx-pZKe2w\n",
      "  Uploaded at: 2017-01-10T00:13:31Z\n",
      "  Caption Preview: so since we seem to be in a trend of going back and using weapons I never made videos for there's one which is\n",
      "\n",
      "Video ID: onGICftOxIM\n",
      "  Uploaded at: 2017-01-07T22:06:42Z\n",
      "  Caption Preview: i'm wearing this hat it only means one thing we have to be playing Call of Duty Black Ops 3 and actually a weapon I have\n",
      "\n",
      "Video ID: pp7BccKNJoQ\n",
      "  Uploaded at: 2017-02-03T00:09:42Z\n",
      "  Caption Preview: Captions disabled or unavailable\n",
      "\n",
      "Video ID: u5qkiuy7NdE\n",
      "  Uploaded at: 2017-02-01T01:18:52Z\n",
      "  Caption Preview: no enjoy to make only how the hell are we to judge nice ass right they're going to persecute oh I saw I was good of time\n",
      "\n",
      "Video ID: zyUXJ3Sf1UU\n",
      "  Uploaded at: 2017-02-27T00:00:04Z\n",
      "  Caption Preview: Oh lazy lazy and what just happened is was unbelievable it is not by any stretch of imagination\n",
      "\n",
      "Video ID: B154_5CKYN4\n",
      "  Uploaded at: 2017-01-08T18:00:00Z\n",
      "  Caption Preview: [Music] [Music] so guys welcome back to another\n",
      "\n",
      "Video ID: CQ-VqkKqvnU\n",
      "  Uploaded at: 2017-02-05T18:00:01Z\n",
      "  Caption Preview: [Music] guys welcome back to another challenge alley episode today my friend master\n",
      "\n",
      "Video ID: CjDcpM-G6DM\n",
      "  Uploaded at: 2017-01-03T18:00:00Z\n",
      "  Caption Preview: [Music] welcome everyone back to some more awesome VR action today we are using the\n",
      "\n",
      "Video ID: Ua4mvGz5Gxw\n",
      "  Uploaded at: 2017-01-26T18:00:00Z\n",
      "  Caption Preview: oh my [Music] God so guys apparently someone here in\n",
      "\n",
      "Video ID: WbFvacx5W08\n",
      "  Uploaded at: 2017-01-10T18:00:01Z\n",
      "  Caption Preview: welcome everyone to another tactical Tuesday hopefully you guys are having an amazing day I'm excited because we're\n",
      "\n",
      "Video ID: ezE9ffg13gU\n",
      "  Uploaded at: 2016-12-29T18:00:02Z\n",
      "  Caption Preview: [Music] so guys who wants to see behind the scenes bloopers never seen before clips\n",
      "\n",
      "Video ID: snRnWILLwDI\n",
      "  Uploaded at: 2017-01-05T18:00:05Z\n",
      "  Caption Preview: welcome everyone today I'm playing Titan full - one of the awesome FPS games at launched last year I played it\n",
      "\n",
      "Video ID: xuD1egfYRVM\n",
      "  Uploaded at: 2017-02-09T18:00:06Z\n",
      "  Caption Preview: [Music] welcome everyone to a challenge early episodes today grants plan of six\n",
      "\n",
      "Video ID: ClzENBgP4mk\n",
      "  Uploaded at: Unknown\n",
      "  Caption Preview: Not Fetched\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "import requests\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "\n",
    "file_path = 'extracted_data.csv'\n",
    "\n",
    "video_ids = []\n",
    "channel_ids = []\n",
    "cluster_ids = []\n",
    "\n",
    "# Read only the first 100 video IDs\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        try:\n",
    "            cluster_ids.append(int(row['cluster']))\n",
    "            channel_ids.append(row['from'])\n",
    "\n",
    "            videos = ast.literal_eval(row['videos'])  # Convert string to list\n",
    "            video_ids.extend(videos)\n",
    "            if len(video_ids) >= 100:\n",
    "                video_ids = video_ids[:100]\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipping row due to error: {e}\")\n",
    "\n",
    "# YouTube API\n",
    "API_KEY = 'AIzaSyDO9W2xxpc7ud4W8N9L06n2Mwv5QkMtoFc'\n",
    "url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "\n",
    "def chunkify(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "timestamps = {}\n",
    "captions = {}\n",
    "\n",
    "print(\"üì° Fetching video metadata and captions...\")\n",
    "\n",
    "for chunk_num, chunk in enumerate(chunkify(video_ids, 50)):\n",
    "    print(f\"üîπ Processing chunk {chunk_num + 1}/{(len(video_ids) - 1) // 50 + 1}...\")\n",
    "    ids_str = \",\".join(chunk)\n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"id\": ids_str,\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error fetching video data: {e}\")\n",
    "        continue\n",
    "\n",
    "    for item in data.get(\"items\", []):\n",
    "        video_id = item[\"id\"]\n",
    "        published_at = item[\"snippet\"][\"publishedAt\"]\n",
    "        timestamps[video_id] = published_at\n",
    "\n",
    "        try:\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
    "            preview = \" \".join([line['text'] for line in transcript[:3]])\n",
    "            captions[video_id] = preview\n",
    "        except TranscriptsDisabled:\n",
    "            captions[video_id] = \"Captions disabled or unavailable\"\n",
    "        except NoTranscriptFound:\n",
    "            captions[video_id] = \"Captions not available in English\"\n",
    "        except Exception as e:\n",
    "            captions[video_id] = f\"Error fetching captions: {str(e)}\"\n",
    "\n",
    "# ‚úÖ Print the result for top 100\n",
    "for vid in video_ids:\n",
    "    print(f\"Video ID: {vid}\")\n",
    "    print(f\"  Uploaded at: {timestamps.get(vid, 'Unknown')}\")\n",
    "    print(f\"  Caption Preview: {captions.get(vid, 'Not Fetched')}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdfc6d8b-9a08-4441-b988-b2c60587be90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì° Fetching video metadata and captions...\n",
      "üîπ Processing chunk 1/20...\n",
      "üîπ Processing chunk 2/20...\n",
      "üîπ Processing chunk 3/20...\n",
      "üîπ Processing chunk 4/20...\n",
      "üîπ Processing chunk 5/20...\n",
      "üîπ Processing chunk 6/20...\n",
      "üîπ Processing chunk 7/20...\n",
      "üîπ Processing chunk 8/20...\n",
      "üîπ Processing chunk 9/20...\n",
      "üîπ Processing chunk 10/20...\n",
      "üîπ Processing chunk 11/20...\n",
      "üîπ Processing chunk 12/20...\n",
      "üîπ Processing chunk 13/20...\n",
      "üîπ Processing chunk 14/20...\n",
      "üîπ Processing chunk 15/20...\n",
      "üîπ Processing chunk 16/20...\n",
      "üîπ Processing chunk 17/20...\n",
      "üîπ Processing chunk 18/20...\n",
      "üîπ Processing chunk 19/20...\n",
      "üîπ Processing chunk 20/20...\n",
      "‚úÖ Output for top 1000 videos saved to video_metadata_1000.txt\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "import requests\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "\n",
    "file_path = 'extracted_data.csv'\n",
    "\n",
    "video_ids = []\n",
    "channel_ids = []\n",
    "cluster_ids = []\n",
    "\n",
    "# ‚úÖ Read up to 1000 video IDs\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        try:\n",
    "            cluster_ids.append(int(row['cluster']))\n",
    "            channel_ids.append(row['from'])\n",
    "\n",
    "            videos = ast.literal_eval(row['videos'])\n",
    "            video_ids.extend(videos)\n",
    "            if len(video_ids) >= 1000:\n",
    "                video_ids = video_ids[:1000]\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipping row due to error: {e}\")\n",
    "\n",
    "# üîë YouTube API\n",
    "API_KEY = 'AIzaSyDO9W2xxpc7ud4W8N9L06n2Mwv5QkMtoFc'\n",
    "url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "\n",
    "def chunkify(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "timestamps = {}\n",
    "captions = {}\n",
    "\n",
    "print(\"üì° Fetching video metadata and captions...\")\n",
    "\n",
    "for chunk_num, chunk in enumerate(chunkify(video_ids, 50)):\n",
    "    print(f\"üîπ Processing chunk {chunk_num + 1}/{(len(video_ids) - 1) // 50 + 1}...\")\n",
    "    ids_str = \",\".join(chunk)\n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"id\": ids_str,\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error fetching video data: {e}\")\n",
    "        continue\n",
    "\n",
    "    for item in data.get(\"items\", []):\n",
    "        video_id = item[\"id\"]\n",
    "        published_at = item[\"snippet\"][\"publishedAt\"]\n",
    "        timestamps[video_id] = published_at\n",
    "\n",
    "        try:\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
    "            preview = \" \".join([line['text'] for line in transcript[:3]])\n",
    "            captions[video_id] = preview\n",
    "        except TranscriptsDisabled:\n",
    "            captions[video_id] = \"Captions disabled or unavailable\"\n",
    "        except NoTranscriptFound:\n",
    "            captions[video_id] = \"Captions not available in English\"\n",
    "        except Exception as e:\n",
    "            captions[video_id] = f\"Error fetching captions: {str(e)}\"\n",
    "\n",
    "# ‚úÖ Save to file instead of printing\n",
    "with open(\"video_metadata_1000.txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for vid in video_ids:\n",
    "        out_file.write(f\"Video ID: {vid}\\n\")\n",
    "        out_file.write(f\"  Uploaded at: {timestamps.get(vid, 'Unknown')}\\n\")\n",
    "        out_file.write(f\"  Caption Preview: {captions.get(vid, 'Not Fetched')}\\n\\n\")\n",
    "\n",
    "print(\"‚úÖ Output for top 1000 videos saved to video_metadata_1000.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2aa0165-7e04-4c56-9b7f-0102067fb99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì° Fetching video metadata and captions (1000 to 2000)...\n",
      "üîπ Processing chunk 1/20...\n",
      "üîπ Processing chunk 2/20...\n",
      "üîπ Processing chunk 3/20...\n",
      "üîπ Processing chunk 4/20...\n",
      "üîπ Processing chunk 5/20...\n",
      "üîπ Processing chunk 6/20...\n",
      "üîπ Processing chunk 7/20...\n",
      "üîπ Processing chunk 8/20...\n",
      "üîπ Processing chunk 9/20...\n",
      "üîπ Processing chunk 10/20...\n",
      "üîπ Processing chunk 11/20...\n",
      "üîπ Processing chunk 12/20...\n",
      "üîπ Processing chunk 13/20...\n",
      "üîπ Processing chunk 14/20...\n",
      "üîπ Processing chunk 15/20...\n",
      "üîπ Processing chunk 16/20...\n",
      "üîπ Processing chunk 17/20...\n",
      "üîπ Processing chunk 18/20...\n",
      "üîπ Processing chunk 19/20...\n",
      "üîπ Processing chunk 20/20...\n",
      "‚úÖ Output for video IDs 1000 to 2000 saved to video_metadata_1000_to_2000.txt\n"
     ]
    }
   ],
   "source": [
    "#from 1000 to 2000 video id \n",
    "import csv\n",
    "import ast\n",
    "import requests\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "\n",
    "file_path = 'extracted_data.csv'\n",
    "\n",
    "video_ids = []\n",
    "channel_ids = []\n",
    "cluster_ids = []\n",
    "\n",
    "# ‚úÖ Skip first 1000 and collect next 1000 video IDs\n",
    "skipped = 0\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        try:\n",
    "            cluster_ids.append(int(row['cluster']))\n",
    "            channel_ids.append(row['from'])\n",
    "\n",
    "            videos = ast.literal_eval(row['videos'])\n",
    "            if skipped + len(videos) <= 1000:\n",
    "                skipped += len(videos)\n",
    "                continue  # skip\n",
    "            elif skipped < 1000:\n",
    "                # Partially skip and take the remaining\n",
    "                take = 1000 - skipped\n",
    "                video_ids.extend(videos[take:])\n",
    "                skipped = 1000\n",
    "            else:\n",
    "                video_ids.extend(videos)\n",
    "\n",
    "            if len(video_ids) >= 1000:\n",
    "                video_ids = video_ids[:1000]\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipping row due to error: {e}\")\n",
    "\n",
    "# üîë YouTube API\n",
    "API_KEY = 'AIzaSyDO9W2xxpc7ud4W8N9L06n2Mwv5QkMtoFc'\n",
    "url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "\n",
    "def chunkify(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "timestamps = {}\n",
    "captions = {}\n",
    "\n",
    "print(\"üì° Fetching video metadata and captions (1000 to 2000)...\")\n",
    "\n",
    "for chunk_num, chunk in enumerate(chunkify(video_ids, 50)):\n",
    "    print(f\"üîπ Processing chunk {chunk_num + 1}/{(len(video_ids) - 1) // 50 + 1}...\")\n",
    "    ids_str = \",\".join(chunk)\n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"id\": ids_str,\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error fetching video data: {e}\")\n",
    "        continue\n",
    "\n",
    "    for item in data.get(\"items\", []):\n",
    "        video_id = item[\"id\"]\n",
    "        published_at = item[\"snippet\"][\"publishedAt\"]\n",
    "        timestamps[video_id] = published_at\n",
    "\n",
    "        try:\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
    "            preview = \" \".join([line['text'] for line in transcript[:3]])\n",
    "            captions[video_id] = preview\n",
    "        except TranscriptsDisabled:\n",
    "            captions[video_id] = \"Captions disabled or unavailable\"\n",
    "        except NoTranscriptFound:\n",
    "            captions[video_id] = \"Captions not available in English\"\n",
    "        except Exception as e:\n",
    "            captions[video_id] = f\"Error fetching captions: {str(e)}\"\n",
    "\n",
    "# ‚úÖ Save output to a new file for 1000-2000\n",
    "with open(\"video_metadata_1000_to_2000.txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for vid in video_ids:\n",
    "        out_file.write(f\"Video ID: {vid}\\n\")\n",
    "        out_file.write(f\"  Uploaded at: {timestamps.get(vid, 'Unknown')}\\n\")\n",
    "        out_file.write(f\"  Caption Preview: {captions.get(vid, 'Not Fetched')}\\n\\n\")\n",
    "\n",
    "print(\"‚úÖ Output for video IDs 1000 to 2000 saved to video_metadata_1000_to_2000.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ac6e8a0-a863-4b45-82e0-7755d9fc4358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Total video IDs collected after skipping 2000: 5614\n",
      "üì° Fetching video metadata and captions (2000 onwards)...\n",
      "üîπ Processing chunk 1/113...\n",
      "üîπ Processing chunk 2/113...\n",
      "üîπ Processing chunk 3/113...\n",
      "üîπ Processing chunk 4/113...\n",
      "üîπ Processing chunk 5/113...\n",
      "üîπ Processing chunk 6/113...\n",
      "üîπ Processing chunk 7/113...\n",
      "üîπ Processing chunk 8/113...\n",
      "üîπ Processing chunk 9/113...\n",
      "üîπ Processing chunk 10/113...\n",
      "üîπ Processing chunk 11/113...\n",
      "üîπ Processing chunk 12/113...\n",
      "üîπ Processing chunk 13/113...\n",
      "üîπ Processing chunk 14/113...\n",
      "üîπ Processing chunk 15/113...\n",
      "üîπ Processing chunk 16/113...\n",
      "üîπ Processing chunk 17/113...\n",
      "üîπ Processing chunk 18/113...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     68\u001b[39m timestamps[video_id] = published_at\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     transcript = \u001b[43mYouTubeTranscriptApi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_transcript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43men\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     preview = \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join([line[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m transcript[:\u001b[32m3\u001b[39m]])\n\u001b[32m     73\u001b[39m     captions[video_id] = preview\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/youtube_transcript_api/_api.py:306\u001b[39m, in \u001b[36mYouTubeTranscriptApi.get_transcript\u001b[39m\u001b[34m(cls, video_id, languages, proxies, cookies, preserve_formatting)\u001b[39m\n\u001b[32m    298\u001b[39m warnings.warn(\n\u001b[32m    299\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m`get_transcript` is deprecated and will be removed in a future version. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    300\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mUse the `fetch` method instead!\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    301\u001b[39m     \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    302\u001b[39m )\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(video_id, \u001b[38;5;28mstr\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33m`video_id` must be a string\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlist_transcripts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcookies\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m     .find_transcript(languages)\n\u001b[32m    308\u001b[39m     .fetch(preserve_formatting=preserve_formatting)\n\u001b[32m    309\u001b[39m     .to_raw_data()\n\u001b[32m    310\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/youtube_transcript_api/_api.py:207\u001b[39m, in \u001b[36mYouTubeTranscriptApi.list_transcripts\u001b[39m\u001b[34m(cls, video_id, proxies, cookies)\u001b[39m\n\u001b[32m    199\u001b[39m         proxy_config = GenericProxyConfig(\n\u001b[32m    200\u001b[39m             http_url=proxies.get(\u001b[33m\"\u001b[39m\u001b[33mhttp\u001b[39m\u001b[33m\"\u001b[39m), https_url=proxies.get(\u001b[33m\"\u001b[39m\u001b[33mhttps\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    201\u001b[39m         )\n\u001b[32m    203\u001b[39m ytt_api = YouTubeTranscriptApi(\n\u001b[32m    204\u001b[39m     proxy_config=proxy_config,\n\u001b[32m    205\u001b[39m     cookie_path=Path(cookies) \u001b[38;5;28;01mif\u001b[39;00m cookies \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    206\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mytt_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/youtube_transcript_api/_api.py:135\u001b[39m, in \u001b[36mYouTubeTranscriptApi.list\u001b[39m\u001b[34m(self, video_id)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mlist\u001b[39m(\n\u001b[32m     85\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     86\u001b[39m     video_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m     87\u001b[39m ) -> TranscriptList:\n\u001b[32m     88\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[33;03m    Retrieves the list of transcripts which are available for a given video. It\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[33;03m    returns a `TranscriptList` object which is iterable and provides methods to\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    133\u001b[39m \u001b[33;03m        Make sure that this is the actual ID, NOT the full URL to the video!\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/youtube_transcript_api/_transcripts.py:352\u001b[39m, in \u001b[36mTranscriptListFetcher.fetch\u001b[39m\u001b[34m(self, video_id)\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, video_id: \u001b[38;5;28mstr\u001b[39m) -> TranscriptList:\n\u001b[32m    349\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m TranscriptList.build(\n\u001b[32m    350\u001b[39m         \u001b[38;5;28mself\u001b[39m._http_client,\n\u001b[32m    351\u001b[39m         video_id,\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_captions_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    353\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/youtube_transcript_api/_transcripts.py:358\u001b[39m, in \u001b[36mTranscriptListFetcher._fetch_captions_json\u001b[39m\u001b[34m(self, video_id, try_number)\u001b[39m\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_fetch_captions_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, video_id: \u001b[38;5;28mstr\u001b[39m, try_number: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m) -> Dict:\n\u001b[32m    356\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    357\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._extract_captions_json(\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_video_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m, video_id\n\u001b[32m    359\u001b[39m         )\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m RequestBlocked \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m    361\u001b[39m         retries = (\n\u001b[32m    362\u001b[39m             \u001b[32m0\u001b[39m\n\u001b[32m    363\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._proxy_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    364\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._proxy_config.retries_when_blocked\n\u001b[32m    365\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/youtube_transcript_api/_transcripts.py:428\u001b[39m, in \u001b[36mTranscriptListFetcher._fetch_video_html\u001b[39m\u001b[34m(self, video_id)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_fetch_video_html\u001b[39m(\u001b[38;5;28mself\u001b[39m, video_id: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m     html = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    429\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33maction=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://consent.youtube.com/s\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m html:\n\u001b[32m    430\u001b[39m         \u001b[38;5;28mself\u001b[39m._create_consent_cookie(html, video_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/youtube_transcript_api/_transcripts.py:437\u001b[39m, in \u001b[36mTranscriptListFetcher._fetch_html\u001b[39m\u001b[34m(self, video_id)\u001b[39m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_fetch_html\u001b[39m(\u001b[38;5;28mself\u001b[39m, video_id: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_http_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWATCH_URL\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m unescape(_raise_http_errors(response, video_id).text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/requests/sessions.py:602\u001b[39m, in \u001b[36mSession.get\u001b[39m\u001b[34m(self, url, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[32m    595\u001b[39m \n\u001b[32m    596\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[33;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m         \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.is_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.proxy_is_verified:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/urllib3/connection.py:741\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    738\u001b[39m     \u001b[38;5;66;03m# Remove trailing '.' from fqdn hostnames to allow certificate validation\u001b[39;00m\n\u001b[32m    739\u001b[39m     server_hostname_rm_dot = server_hostname.rstrip(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m     sock_and_verified = \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    743\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    744\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    745\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    749\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    755\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    757\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    759\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = sock_and_verified.socket\n\u001b[32m    761\u001b[39m \u001b[38;5;66;03m# If an error occurs during connection/handshake we may need to release\u001b[39;00m\n\u001b[32m    762\u001b[39m \u001b[38;5;66;03m# our lock so another connection can probe the origin.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/urllib3/connection.py:920\u001b[39m, in \u001b[36m_ssl_wrap_socket_and_match_hostname\u001b[39m\u001b[34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[39m\n\u001b[32m    917\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[32m    918\u001b[39m         server_hostname = normalized\n\u001b[32m--> \u001b[39m\u001b[32m920\u001b[39m ssl_sock = \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/urllib3/util/ssl_.py:460\u001b[39m, in \u001b[36mssl_wrap_socket\u001b[39m\u001b[34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[39m\n\u001b[32m    456\u001b[39m         context.load_cert_chain(certfile, keyfile, key_password)\n\u001b[32m    458\u001b[39m context.set_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m ssl_sock = \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/urllib3/util/ssl_.py:504\u001b[39m, in \u001b[36m_ssl_wrap_socket_impl\u001b[39m\u001b[34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[39m\n\u001b[32m    501\u001b[39m     SSLTransport._validate_ssl_context_for_tls_in_tls(ssl_context)\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/ssl.py:455\u001b[39m, in \u001b[36mSSLContext.wrap_socket\u001b[39m\u001b[34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    450\u001b[39m                 do_handshake_on_connect=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    451\u001b[39m                 suppress_ragged_eofs=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    452\u001b[39m                 server_hostname=\u001b[38;5;28;01mNone\u001b[39;00m, session=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msslsocket_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/ssl.py:1041\u001b[39m, in \u001b[36mSSLSocket._create\u001b[39m\u001b[34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[39m\n\u001b[32m   1038\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m timeout == \u001b[32m0.0\u001b[39m:\n\u001b[32m   1039\u001b[39m                 \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[32m   1040\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1041\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1043\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/ssl.py:1319\u001b[39m, in \u001b[36mSSLSocket.do_handshake\u001b[39m\u001b[34m(self, block)\u001b[39m\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout == \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[32m   1318\u001b[39m         \u001b[38;5;28mself\u001b[39m.settimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1321\u001b[39m     \u001b[38;5;28mself\u001b[39m.settimeout(timeout)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "import requests\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "\n",
    "file_path = 'extracted_data.csv'\n",
    "\n",
    "video_ids = []\n",
    "channel_ids = []\n",
    "cluster_ids = []\n",
    "\n",
    "# ‚úÖ Skip first 2000 video IDs and collect the rest\n",
    "skipped = 0\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        try:\n",
    "            cluster_ids.append(int(row['cluster']))\n",
    "            channel_ids.append(row['from'])\n",
    "\n",
    "            videos = ast.literal_eval(row['videos'])\n",
    "            if skipped + len(videos) <= 2000:\n",
    "                skipped += len(videos)\n",
    "                continue  # skip\n",
    "            elif skipped < 2000:\n",
    "                take = 2000 - skipped\n",
    "                video_ids.extend(videos[take:])\n",
    "                skipped = 2000\n",
    "            else:\n",
    "                video_ids.extend(videos)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipping row due to error: {e}\")\n",
    "\n",
    "print(f\"üì¶ Total video IDs collected after skipping 2000: {len(video_ids)}\")\n",
    "\n",
    "# üîë YouTube API\n",
    "API_KEY = 'AIzaSyDO9W2xxpc7ud4W8N9L06n2Mwv5QkMtoFc'\n",
    "url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "\n",
    "def chunkify(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "timestamps = {}\n",
    "captions = {}\n",
    "\n",
    "print(\"üì° Fetching video metadata and captions (2000 onwards)...\")\n",
    "\n",
    "for chunk_num, chunk in enumerate(chunkify(video_ids, 50)):\n",
    "    print(f\"üîπ Processing chunk {chunk_num + 1}/{(len(video_ids) - 1) // 50 + 1}...\")\n",
    "    ids_str = \",\".join(chunk)\n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"id\": ids_str,\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error fetching video data: {e}\")\n",
    "        continue\n",
    "\n",
    "    for item in data.get(\"items\", []):\n",
    "        video_id = item[\"id\"]\n",
    "        published_at = item[\"snippet\"][\"publishedAt\"]\n",
    "        timestamps[video_id] = published_at\n",
    "\n",
    "        try:\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
    "            preview = \" \".join([line['text'] for line in transcript[:3]])\n",
    "            captions[video_id] = preview\n",
    "        except TranscriptsDisabled:\n",
    "            captions[video_id] = \"Captions disabled or unavailable\"\n",
    "        except NoTranscriptFound:\n",
    "            captions[video_id] = \"Captions not available in English\"\n",
    "        except Exception as e:\n",
    "            captions[video_id] = f\"Error fetching captions: {str(e)}\"\n",
    "\n",
    "# ‚úÖ Save output to a new file for 2000 onward\n",
    "with open(\"video_metadata_2000_onwards.txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for vid in video_ids:\n",
    "        out_file.write(f\"Video ID: {vid}\\n\")\n",
    "        out_file.write(f\"  Uploaded at: {timestamps.get(vid, 'Unknown')}\\n\")\n",
    "        out_file.write(f\"  Caption Preview: {captions.get(vid, 'Not Fetched')}\\n\\n\")\n",
    "\n",
    "print(\"‚úÖ Output for video IDs from 2000 onwards saved to video_metadata_2000_onwards.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d656cf64-a291-40d0-aada-eb64cc314847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total video IDs collected from first 1000 rows: 2147\n",
      "üì° Fetching video metadata and captions...\n",
      "üîπ Processing chunk 1/43...\n",
      "üîπ Processing chunk 2/43...\n",
      "üîπ Processing chunk 3/43...\n",
      "üîπ Processing chunk 4/43...\n",
      "üîπ Processing chunk 5/43...\n",
      "üîπ Processing chunk 6/43...\n",
      "üîπ Processing chunk 7/43...\n",
      "üîπ Processing chunk 8/43...\n",
      "üîπ Processing chunk 9/43...\n",
      "üîπ Processing chunk 10/43...\n",
      "üîπ Processing chunk 11/43...\n",
      "üîπ Processing chunk 12/43...\n",
      "üîπ Processing chunk 13/43...\n",
      "üîπ Processing chunk 14/43...\n",
      "üîπ Processing chunk 15/43...\n",
      "üîπ Processing chunk 16/43...\n",
      "üîπ Processing chunk 17/43...\n",
      "üîπ Processing chunk 18/43...\n",
      "üîπ Processing chunk 19/43...\n",
      "üîπ Processing chunk 20/43...\n",
      "üîπ Processing chunk 21/43...\n",
      "üîπ Processing chunk 22/43...\n",
      "üîπ Processing chunk 23/43...\n",
      "üîπ Processing chunk 24/43...\n",
      "üîπ Processing chunk 25/43...\n",
      "üîπ Processing chunk 26/43...\n",
      "üîπ Processing chunk 27/43...\n",
      "üîπ Processing chunk 28/43...\n",
      "üîπ Processing chunk 29/43...\n",
      "üîπ Processing chunk 30/43...\n",
      "üîπ Processing chunk 31/43...\n",
      "üîπ Processing chunk 32/43...\n",
      "üîπ Processing chunk 33/43...\n",
      "üîπ Processing chunk 34/43...\n",
      "üîπ Processing chunk 35/43...\n",
      "üîπ Processing chunk 36/43...\n",
      "üîπ Processing chunk 37/43...\n",
      "üîπ Processing chunk 38/43...\n",
      "üîπ Processing chunk 39/43...\n",
      "üîπ Processing chunk 40/43...\n",
      "üîπ Processing chunk 41/43...\n",
      "üîπ Processing chunk 42/43...\n",
      "üîπ Processing chunk 43/43...\n",
      "‚úÖ Output for rows 1 to 1000 saved to video_metadata_rows_1_to_1000.txt\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "import requests\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "\n",
    "file_path = 'extracted_data.csv'\n",
    "\n",
    "video_ids = []\n",
    "channel_ids = []\n",
    "cluster_ids = []\n",
    "\n",
    "# ‚úÖ Read exactly first 1000 rows\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for i, row in enumerate(reader):\n",
    "        if i >= 1000:\n",
    "            break\n",
    "        try:\n",
    "            cluster_ids.append(int(row['cluster']))\n",
    "            channel_ids.append(row['from'])\n",
    "\n",
    "            videos = ast.literal_eval(row['videos'])\n",
    "            video_ids.extend(videos)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipping row {i} due to error: {e}\")\n",
    "\n",
    "print(f\"üìä Total video IDs collected from first 1000 rows: {len(video_ids)}\")\n",
    "\n",
    "# üîë YouTube API setup\n",
    "API_KEY = 'AIzaSyDO9W2xxpc7ud4W8N9L06n2Mwv5QkMtoFc'\n",
    "url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "\n",
    "def chunkify(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "timestamps = {}\n",
    "captions = {}\n",
    "\n",
    "print(\"üì° Fetching video metadata and captions...\")\n",
    "\n",
    "for chunk_num, chunk in enumerate(chunkify(video_ids, 50)):\n",
    "    print(f\"üîπ Processing chunk {chunk_num + 1}/{(len(video_ids) - 1) // 50 + 1}...\")\n",
    "    ids_str = \",\".join(chunk)\n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"id\": ids_str,\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error fetching video data: {e}\")\n",
    "        continue\n",
    "\n",
    "    for item in data.get(\"items\", []):\n",
    "        video_id = item[\"id\"]\n",
    "        published_at = item[\"snippet\"][\"publishedAt\"]\n",
    "        timestamps[video_id] = published_at\n",
    "\n",
    "        try:\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
    "            preview = \" \".join([line['text'] for line in transcript[:3]])\n",
    "            captions[video_id] = preview\n",
    "        except TranscriptsDisabled:\n",
    "            captions[video_id] = \"Captions disabled or unavailable\"\n",
    "        except NoTranscriptFound:\n",
    "            captions[video_id] = \"Captions not available in English\"\n",
    "        except Exception as e:\n",
    "            captions[video_id] = f\"Error fetching captions: {str(e)}\"\n",
    "\n",
    "# ‚úÖ Save to file\n",
    "with open(\"video_metadata_rows_1_to_1000.txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for vid in video_ids:\n",
    "        out_file.write(f\"Video ID: {vid}\\n\")\n",
    "        out_file.write(f\"  Uploaded at: {timestamps.get(vid, 'Unknown')}\\n\")\n",
    "        out_file.write(f\"  Caption Preview: {captions.get(vid, 'Not Fetched')}\\n\\n\")\n",
    "\n",
    "print(\"‚úÖ Output for rows 1 to 1000 saved to video_metadata_rows_1_to_1000.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
