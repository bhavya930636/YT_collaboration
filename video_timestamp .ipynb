{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cd605e2-9f53-4fe5-862e-0e875f3f51fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/bhoomi/miniconda3/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bhoomi/miniconda3/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bhoomi/miniconda3/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bhoomi/miniconda3/lib/python3.12/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bhoomi/miniconda3/lib/python3.12/site-packages (from requests) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e06702-00d9-416b-b897-54c348d5f969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected video IDs: ['khyg5YJlLPI', 'qP8HTuEu7BY', 'On4GE5hAU8s']\n",
      "\n",
      "Video Upload Timestamps:\n",
      "khyg5YJlLPI: 2017-01-11T14:00:04Z\n",
      "qP8HTuEu7BY: 2017-01-09T22:00:01Z\n",
      "On4GE5hAU8s: 2017-02-11T17:10:18Z\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import requests\n",
    "\n",
    "# Sample dictionary\n",
    "data = {\n",
    "    \"cluster\": [220, 11097, 5556],\n",
    "    \"from\": [\"UCq6VFHwMzcMXbuKyG7SQYIg\", \"UCq6VFHwMzcMXbuKyG7SQYIg\", \"UClpEE-Led9ZK0GJQKvU--3Q\"],\n",
    "    \"videos\": ['[\"khyg5YJlLPI\"]', '[\"qP8HTuEu7BY\"]', '[\"On4GE5hAU8s\"]']\n",
    "}\n",
    "\n",
    "# Extract video IDs from the dictionary\n",
    "video_ids = []\n",
    "\n",
    "for v in data[\"videos\"]:\n",
    "    try:\n",
    "        ids = ast.literal_eval(v)\n",
    "        video_ids.extend(ids)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping due to error: {e}\")\n",
    "\n",
    "print(\"Collected video IDs:\", video_ids)\n",
    "\n",
    "# Query YouTube API\n",
    "API_KEY = 'AIzaSyDO9W2xxpc7ud4W8N9L06n2Mwv5QkMtoFc'  # Replace with your actual key\n",
    "url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "\n",
    "def chunkify(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "timestamps = {}\n",
    "\n",
    "for chunk in chunkify(video_ids, 50):\n",
    "    ids_str = \",\".join(chunk)\n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"id\": ids_str,\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    for item in data.get(\"items\", []):\n",
    "        video_id = item[\"id\"]\n",
    "        published_at = item[\"snippet\"][\"publishedAt\"]\n",
    "        timestamps[video_id] = published_at\n",
    "\n",
    "print(\"\\nVideo Upload Timestamps:\")\n",
    "for vid, ts in timestamps.items():\n",
    "    print(f\"{vid}: {ts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "103b9bec-7ef3-4294-82d3-1f91f5f3bcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected video IDs: ['khyg5YJlLPI', 'qP8HTuEu7BY', 'On4GE5hAU8s']\n",
      "\n",
      "Video Upload Info with Captions:\n",
      "\n",
      "Video ID: khyg5YJlLPI\n",
      "  Uploaded at: 2017-01-11T14:00:04Z\n",
      "  Caption Preview: what if Bruce Wayne didn't survive in the alley what if his father did who would become\n",
      "\n",
      "Video ID: qP8HTuEu7BY\n",
      "  Uploaded at: 2017-01-09T22:00:01Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: On4GE5hAU8s\n",
      "  Uploaded at: 2017-02-11T17:10:18Z\n",
      "  Caption Preview: Captions disabled or unavailable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import requests\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "\n",
    "# Sample dictionary\n",
    "data = {\n",
    "    \"cluster\": [220, 11097, 5556],\n",
    "    \"from\": [\"UCq6VFHwMzcMXbuKyG7SQYIg\", \"UCq6VFHwMzcMXbuKyG7SQYIg\", \"UClpEE-Led9ZK0GJQKvU--3Q\"],\n",
    "    \"videos\": ['[\"khyg5YJlLPI\"]', '[\"qP8HTuEu7BY\"]', '[\"On4GE5hAU8s\"]']\n",
    "}\n",
    "\n",
    "# Extract video IDs\n",
    "video_ids = []\n",
    "for v in data[\"videos\"]:\n",
    "    try:\n",
    "        ids = ast.literal_eval(v)\n",
    "        video_ids.extend(ids)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping due to error: {e}\")\n",
    "\n",
    "print(\"Collected video IDs:\", video_ids)\n",
    "\n",
    "# YouTube API config\n",
    "API_KEY = 'AIzaSyDO9W2xxpc7ud4W8N9L06n2Mwv5QkMtoFc'  # Replace with your real API key\n",
    "url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "\n",
    "def chunkify(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "timestamps = {}\n",
    "captions = {}\n",
    "\n",
    "for chunk in chunkify(video_ids, 50):\n",
    "    ids_str = \",\".join(chunk)\n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"id\": ids_str,\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    for item in data.get(\"items\", []):\n",
    "        video_id = item[\"id\"]\n",
    "        published_at = item[\"snippet\"][\"publishedAt\"]\n",
    "        timestamps[video_id] = published_at\n",
    "\n",
    "        # Fetch captions\n",
    "        try:\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
    "            preview = \" \".join([line['text'] for line in transcript[:3]])\n",
    "            captions[video_id] = preview\n",
    "        except TranscriptsDisabled:\n",
    "            captions[video_id] = \"Captions disabled or unavailable\"\n",
    "        except NoTranscriptFound:\n",
    "            captions[video_id] = \"Captions not available in English\"\n",
    "        except Exception as e:\n",
    "            captions[video_id] = f\"Error fetching captions\"\n",
    "\n",
    "# Final output\n",
    "print(\"\\nVideo Upload Info with Captions:\\n\")\n",
    "for vid in video_ids:\n",
    "    print(f\"Video ID: {vid}\")\n",
    "    print(f\"  Uploaded at: {timestamps.get(vid, 'Unknown')}\")\n",
    "    print(f\"  Caption Preview: {captions.get(vid)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f784b610-89b5-49fd-b3f6-2d471098b0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/bhoomi/miniconda3/lib/python3.12/site-packages (2.32.3)\n",
      "Collecting youtube-transcript-api\n",
      "  Downloading youtube_transcript_api-1.0.3-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bhoomi/miniconda3/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bhoomi/miniconda3/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bhoomi/miniconda3/lib/python3.12/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bhoomi/miniconda3/lib/python3.12/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /home/bhoomi/miniconda3/lib/python3.12/site-packages (from youtube-transcript-api) (0.7.1)\n",
      "Downloading youtube_transcript_api-1.0.3-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: youtube-transcript-api\n",
      "Successfully installed youtube-transcript-api-1.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install requests youtube-transcript-api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aeb13e1-d07d-4916-aded-31a7a00ed553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted data written to: extracted_data.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "input_file = 'data.txt'         # Your input file name\n",
    "output_file = 'extracted_data.csv'  # You can also change to .txt if needed\n",
    "\n",
    "# Open and read TSV input\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8', newline='') as outfile:\n",
    "    reader = csv.DictReader(infile, delimiter='\\t')\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    # Write header\n",
    "    writer.writerow(['cluster', 'from', 'videos'])\n",
    "\n",
    "    for row in reader:\n",
    "        cluster = row['cluster'].strip()\n",
    "        frm = row['from'].strip()\n",
    "        videos = row['videos'].strip()\n",
    "\n",
    "        # Only write valid rows\n",
    "        if cluster and frm and videos:\n",
    "            writer.writerow([cluster, frm, videos])\n",
    "\n",
    "print(f\"✅ Extracted data written to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba74cf8c-5890-40b1-bddd-7465fc8b0a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📡 Fetching video metadata and captions...\n",
      "🔹 Processing chunk 1/153...\n",
      "🔹 Processing chunk 2/153...\n",
      "🔹 Processing chunk 3/153...\n",
      "🔹 Processing chunk 4/153...\n",
      "🔹 Processing chunk 5/153...\n",
      "🔹 Processing chunk 6/153...\n",
      "🔹 Processing chunk 7/153...\n",
      "🔹 Processing chunk 8/153...\n",
      "🔹 Processing chunk 9/153...\n",
      "🔹 Processing chunk 10/153...\n",
      "🔹 Processing chunk 11/153...\n",
      "🔹 Processing chunk 12/153...\n",
      "🔹 Processing chunk 13/153...\n",
      "🔹 Processing chunk 14/153...\n",
      "🔹 Processing chunk 15/153...\n",
      "🔹 Processing chunk 16/153...\n",
      "🔹 Processing chunk 17/153...\n",
      "🔹 Processing chunk 18/153...\n",
      "🔹 Processing chunk 19/153...\n",
      "🔹 Processing chunk 20/153...\n",
      "🔹 Processing chunk 21/153...\n",
      "🔹 Processing chunk 22/153...\n",
      "🔹 Processing chunk 23/153...\n",
      "🔹 Processing chunk 24/153...\n",
      "🔹 Processing chunk 25/153...\n",
      "🔹 Processing chunk 26/153...\n",
      "🔹 Processing chunk 27/153...\n",
      "🔹 Processing chunk 28/153...\n",
      "🔹 Processing chunk 29/153...\n",
      "🔹 Processing chunk 30/153...\n",
      "🔹 Processing chunk 31/153...\n",
      "🔹 Processing chunk 32/153...\n",
      "🔹 Processing chunk 33/153...\n",
      "🔹 Processing chunk 34/153...\n",
      "🔹 Processing chunk 35/153...\n",
      "🔹 Processing chunk 36/153...\n",
      "🔹 Processing chunk 37/153...\n",
      "🔹 Processing chunk 38/153...\n",
      "🔹 Processing chunk 39/153...\n",
      "🔹 Processing chunk 40/153...\n",
      "🔹 Processing chunk 41/153...\n",
      "🔹 Processing chunk 42/153...\n",
      "🔹 Processing chunk 43/153...\n",
      "🔹 Processing chunk 44/153...\n",
      "🔹 Processing chunk 45/153...\n",
      "🔹 Processing chunk 46/153...\n",
      "🔹 Processing chunk 47/153...\n",
      "🔹 Processing chunk 48/153...\n",
      "🔹 Processing chunk 49/153...\n",
      "🔹 Processing chunk 50/153...\n",
      "🔹 Processing chunk 51/153...\n",
      "🔹 Processing chunk 52/153...\n",
      "🔹 Processing chunk 53/153...\n",
      "🔹 Processing chunk 54/153...\n",
      "🔹 Processing chunk 55/153...\n",
      "🔹 Processing chunk 56/153...\n",
      "🔹 Processing chunk 57/153...\n",
      "🔹 Processing chunk 58/153...\n",
      "🔹 Processing chunk 59/153...\n",
      "🔹 Processing chunk 60/153...\n",
      "🔹 Processing chunk 61/153...\n",
      "🔹 Processing chunk 62/153...\n",
      "🔹 Processing chunk 63/153...\n",
      "🔹 Processing chunk 64/153...\n",
      "🔹 Processing chunk 65/153...\n",
      "🔹 Processing chunk 66/153...\n",
      "🔹 Processing chunk 67/153...\n",
      "🔹 Processing chunk 68/153...\n",
      "🔹 Processing chunk 69/153...\n",
      "🔹 Processing chunk 70/153...\n",
      "🔹 Processing chunk 71/153...\n",
      "🔹 Processing chunk 72/153...\n",
      "🔹 Processing chunk 73/153...\n",
      "🔹 Processing chunk 74/153...\n",
      "🔹 Processing chunk 75/153...\n",
      "🔹 Processing chunk 76/153...\n",
      "🔹 Processing chunk 77/153...\n",
      "🔹 Processing chunk 78/153...\n",
      "🔹 Processing chunk 79/153...\n",
      "🔹 Processing chunk 80/153...\n",
      "🔹 Processing chunk 81/153...\n",
      "🔹 Processing chunk 82/153...\n",
      "🔹 Processing chunk 83/153...\n",
      "🔹 Processing chunk 84/153...\n",
      "🔹 Processing chunk 85/153...\n",
      "🔹 Processing chunk 86/153...\n",
      "🔹 Processing chunk 87/153...\n",
      "🔹 Processing chunk 88/153...\n",
      "🔹 Processing chunk 89/153...\n",
      "🔹 Processing chunk 90/153...\n",
      "🔹 Processing chunk 91/153...\n",
      "🔹 Processing chunk 92/153...\n",
      "🔹 Processing chunk 93/153...\n",
      "🔹 Processing chunk 94/153...\n",
      "🔹 Processing chunk 95/153...\n",
      "🔹 Processing chunk 96/153...\n",
      "🔹 Processing chunk 97/153...\n",
      "🔹 Processing chunk 98/153...\n",
      "🔹 Processing chunk 99/153...\n",
      "🔹 Processing chunk 100/153...\n",
      "🔹 Processing chunk 101/153...\n",
      "🔹 Processing chunk 102/153...\n",
      "🔹 Processing chunk 103/153...\n",
      "🔹 Processing chunk 104/153...\n",
      "🔹 Processing chunk 105/153...\n",
      "🔹 Processing chunk 106/153...\n",
      "🔹 Processing chunk 107/153...\n",
      "🔹 Processing chunk 108/153...\n",
      "🔹 Processing chunk 109/153...\n",
      "🔹 Processing chunk 110/153...\n",
      "🔹 Processing chunk 111/153...\n",
      "🔹 Processing chunk 112/153...\n",
      "🔹 Processing chunk 113/153...\n",
      "🔹 Processing chunk 114/153...\n",
      "🔹 Processing chunk 115/153...\n",
      "🔹 Processing chunk 116/153...\n",
      "🔹 Processing chunk 117/153...\n",
      "🔹 Processing chunk 118/153...\n",
      "🔹 Processing chunk 119/153...\n",
      "🔹 Processing chunk 120/153...\n",
      "🔹 Processing chunk 121/153...\n",
      "🔹 Processing chunk 122/153...\n",
      "🔹 Processing chunk 123/153...\n",
      "🔹 Processing chunk 124/153...\n",
      "🔹 Processing chunk 125/153...\n",
      "🔹 Processing chunk 126/153...\n",
      "🔹 Processing chunk 127/153...\n",
      "🔹 Processing chunk 128/153...\n",
      "🔹 Processing chunk 129/153...\n",
      "🔹 Processing chunk 130/153...\n",
      "🔹 Processing chunk 131/153...\n",
      "🔹 Processing chunk 132/153...\n",
      "🔹 Processing chunk 133/153...\n",
      "🔹 Processing chunk 134/153...\n",
      "🔹 Processing chunk 135/153...\n",
      "🔹 Processing chunk 136/153...\n",
      "🔹 Processing chunk 137/153...\n",
      "🔹 Processing chunk 138/153...\n",
      "🔹 Processing chunk 139/153...\n",
      "🔹 Processing chunk 140/153...\n",
      "🔹 Processing chunk 141/153...\n",
      "🔹 Processing chunk 142/153...\n",
      "🔹 Processing chunk 143/153...\n",
      "🔹 Processing chunk 144/153...\n",
      "🔹 Processing chunk 145/153...\n",
      "🔹 Processing chunk 146/153...\n",
      "🔹 Processing chunk 147/153...\n",
      "🔹 Processing chunk 148/153...\n",
      "🔹 Processing chunk 149/153...\n",
      "🔹 Processing chunk 150/153...\n",
      "🔹 Processing chunk 151/153...\n",
      "🔹 Processing chunk 152/153...\n",
      "🔹 Processing chunk 153/153...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "import requests\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "\n",
    "# 🔄 Updated file path\n",
    "file_path = 'extracted_data.csv'\n",
    "\n",
    "video_ids = []\n",
    "channel_ids = []\n",
    "cluster_ids = []\n",
    "\n",
    "# 🧾 Reading CSV file\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        try:\n",
    "            cluster_ids.append(int(row['cluster']))\n",
    "            channel_ids.append(row['from'])\n",
    "\n",
    "            videos = ast.literal_eval(row['videos'])  # Convert string to list\n",
    "            video_ids.extend(videos)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipping row due to error: {e}\")\n",
    "\n",
    "# 🔑 YouTube API config\n",
    "API_KEY = 'AIzaSyDO9W2xxpc7ud4W8N9L06n2Mwv5QkMtoFc'\n",
    "url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "\n",
    "def chunkify(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "timestamps = {}\n",
    "captions = {}\n",
    "\n",
    "print(\"📡 Fetching video metadata and captions...\")\n",
    "\n",
    "for chunk_num, chunk in enumerate(chunkify(video_ids, 50)):\n",
    "    print(f\"🔹 Processing chunk {chunk_num + 1}/{(len(video_ids) - 1) // 50 + 1}...\")\n",
    "    ids_str = \",\".join(chunk)\n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"id\": ids_str,\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error fetching video data: {e}\")\n",
    "        continue\n",
    "\n",
    "    for item in data.get(\"items\", []):\n",
    "        video_id = item[\"id\"]\n",
    "        published_at = item[\"snippet\"][\"publishedAt\"]\n",
    "        timestamps[video_id] = published_at\n",
    "\n",
    "        try:\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
    "            preview = \" \".join([line['text'] for line in transcript[:3]])\n",
    "            captions[video_id] = preview\n",
    "        except TranscriptsDisabled:\n",
    "            captions[video_id] = \"Captions disabled or unavailable\"\n",
    "        except NoTranscriptFound:\n",
    "            captions[video_id] = \"Captions not available in English\"\n",
    "        except Exception as e:\n",
    "            captions[video_id] = f\"Error fetching captions: {str(e)}\"\n",
    "\n",
    "# ✅ Final formatted output\n",
    "for vid in video_ids:\n",
    "    print(f\"Video ID: {vid}\")\n",
    "    print(f\"  Uploaded at: {timestamps.get(vid, 'Unknown')}\")\n",
    "    print(f\"  Caption Preview: {captions.get(vid, 'Not Fetched')}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76553b9f-c174-4c24-8c86-5c8743c2a4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📡 Fetching video metadata and captions...\n",
      "🔹 Processing chunk 1/2...\n",
      "🔹 Processing chunk 2/2...\n",
      "Video ID: khyg5YJlLPI\n",
      "  Uploaded at: 2017-01-11T14:00:04Z\n",
      "  Caption Preview: what if Bruce Wayne didn't survive in the alley what if his father did who would become\n",
      "\n",
      "Video ID: qP8HTuEu7BY\n",
      "  Uploaded at: 2017-01-09T22:00:01Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: On4GE5hAU8s\n",
      "  Uploaded at: 2017-02-11T17:10:18Z\n",
      "  Caption Preview: Captions disabled or unavailable\n",
      "\n",
      "Video ID: hAvpa3iyrsM\n",
      "  Uploaded at: 2017-01-16T18:00:39Z\n",
      "  Caption Preview: what's up guys it's your boy Wolfie and marrick and we are joined with Nellis and today we are doing a pack opening\n",
      "\n",
      "Video ID: 4cEkXTIcvH4\n",
      "  Uploaded at: 2017-03-25T11:29:30Z\n",
      "  Caption Preview: what wait got did you actually he did he didn't get M did he\n",
      "\n",
      "Video ID: _kOwJy1fOuQ\n",
      "  Uploaded at: 2017-01-14T19:18:51Z\n",
      "  Caption Preview: is it on the player yeah I just got it it's gone who was it Thiago Silva you're lying no n was it really wait this kind\n",
      "\n",
      "Video ID: WvCaYRRL9F8\n",
      "  Uploaded at: 2017-02-12T19:25:05Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: C7Waq5ZgqGU\n",
      "  Uploaded at: Unknown\n",
      "  Caption Preview: Not Fetched\n",
      "\n",
      "Video ID: VNXJbRvu8zE\n",
      "  Uploaded at: 2017-01-24T19:24:03Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: 2LAQMtZKpw8\n",
      "  Uploaded at: 2017-01-20T21:00:00Z\n",
      "  Caption Preview: Jurassic World - The Game Episode 178 Stegoceratops Level 40 Dinosaurs Ludia vs Indominus Gameplay going to be the fourth episode in my new format where I'm running you get through\n",
      "\n",
      "Video ID: KxWE0NJ96eg\n",
      "  Uploaded at: 2017-02-20T21:00:01Z\n",
      "  Caption Preview: Captions disabled or unavailable\n",
      "\n",
      "Video ID: hnbtt8DTrXI\n",
      "  Uploaded at: 2017-01-27T21:00:01Z\n",
      "  Caption Preview: okay guys welcome back today we're going to be hitting level 74 that's going to be awesome getting really near 75 I\n",
      "\n",
      "Video ID: pz8DhJZxbhQ\n",
      "  Uploaded at: 2017-02-27T21:00:01Z\n",
      "  Caption Preview: ok guys we'll come back Jurassic World - The Game Episode 196 New Hybrid Unayrhynchus Dinosaurs Ludia vs Indominus Rex episode drastic world the game we have also battle today we're going to have\n",
      "\n",
      "Video ID: RJir4ghyTDo\n",
      "  Uploaded at: 2017-03-12T06:57:23Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: bWtjQIbp08c\n",
      "  Uploaded at: 2017-03-08T21:30:00Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: Vur2PfNlI_Q\n",
      "  Uploaded at: Unknown\n",
      "  Caption Preview: Not Fetched\n",
      "\n",
      "Video ID: dB50g5a1Dx8\n",
      "  Uploaded at: Unknown\n",
      "  Caption Preview: Not Fetched\n",
      "\n",
      "Video ID: EQtp4c2QBqk\n",
      "  Uploaded at: Unknown\n",
      "  Caption Preview: Not Fetched\n",
      "\n",
      "Video ID: syFKneGDzEQ\n",
      "  Uploaded at: 2017-02-01T14:00:08Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: AVNlxjv7Yvo\n",
      "  Uploaded at: 2017-03-14T22:00:00Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: 9Ewtl2sASU4\n",
      "  Uploaded at: 2017-02-09T02:56:08Z\n",
      "  Caption Preview: [Applause] [Music] [Applause]\n",
      "\n",
      "Video ID: 3WIrb4FvjhI\n",
      "  Uploaded at: 2017-01-29T18:34:13Z\n",
      "  Caption Preview: what's good YouTube it's Aaron here at the house of champions and we special guest today doing the commentary what's\n",
      "\n",
      "Video ID: dRDEBaMa2YA\n",
      "  Uploaded at: 2016-12-29T19:35:22Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: LSj5HI2qesw\n",
      "  Uploaded at: 2017-03-21T20:00:01Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: ECbwCuN-5FE\n",
      "  Uploaded at: 2017-03-03T14:40:57Z\n",
      "  Caption Preview: [Music] ladies and gentlemen welcome to son of a beach tonight's guests are Raa ab and\n",
      "\n",
      "Video ID: 0Fj5uDNGyg8\n",
      "  Uploaded at: 2017-03-03T17:01:00Z\n",
      "  Caption Preview: right guys so today I'm here with Harry Cal and Theo he's always here but we're going to be doing uh different\n",
      "\n",
      "Video ID: 8NGNWUcjmxQ\n",
      "  Uploaded at: 2017-03-12T17:00:05Z\n",
      "  Caption Preview: I'm ready I'm ready [Music] I'm it is back [ __ ] cyon FC\n",
      "\n",
      "Video ID: oF_Rf5t6boA\n",
      "  Uploaded at: Unknown\n",
      "  Caption Preview: Not Fetched\n",
      "\n",
      "Video ID: tdsNqpOLbA0\n",
      "  Uploaded at: 2017-02-25T13:37:07Z\n",
      "  Caption Preview: Captions disabled or unavailable\n",
      "\n",
      "Video ID: 5qfmGkAECFk\n",
      "  Uploaded at: 2017-03-27T17:30:00Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: 5iek4LiMRx8\n",
      "  Uploaded at: 2017-03-01T01:56:19Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: 8g6ndDE0MYA\n",
      "  Uploaded at: 2017-01-28T09:01:02Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: OZXkX368Nsg\n",
      "  Uploaded at: 2017-01-29T12:00:01Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: mze0QYK4GQE\n",
      "  Uploaded at: 2017-01-28T10:00:32Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: rULUEf9faJo\n",
      "  Uploaded at: 2017-01-26T16:00:02Z\n",
      "  Caption Preview: Captions disabled or unavailable\n",
      "\n",
      "Video ID: SgH19fwSjKo\n",
      "  Uploaded at: 2017-02-10T08:34:54Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: 7VQEk6SSAjU\n",
      "  Uploaded at: 2017-02-02T17:36:39Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: aCDN1pAEJaQ\n",
      "  Uploaded at: 2017-03-20T21:00:00Z\n",
      "  Caption Preview: hello and welcome back to most amazing top 10 how are you all doing I'm Rebecca felgate and today's episode of most\n",
      "\n",
      "Video ID: HJv-ZTMd7k8\n",
      "  Uploaded at: 2017-01-21T18:51:46Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: f9fnYaT_X-k\n",
      "  Uploaded at: 2017-03-15T13:30:01Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: t1dE2rNOF9s\n",
      "  Uploaded at: 2017-03-08T13:30:00Z\n",
      "  Caption Preview: Captions disabled or unavailable\n",
      "\n",
      "Video ID: bhdwPrUcO_A\n",
      "  Uploaded at: 2017-02-06T18:30:00Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: 96yuR2Yi3m8\n",
      "  Uploaded at: 2017-03-05T13:30:00Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: 3Z0GQ2Ma8O8\n",
      "  Uploaded at: 2017-03-20T12:32:30Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: 57iu7iXWY04\n",
      "  Uploaded at: 2017-03-07T12:30:17Z\n",
      "  Caption Preview: Captions not available in English\n",
      "\n",
      "Video ID: -x3LutVAV14\n",
      "  Uploaded at: Unknown\n",
      "  Caption Preview: Not Fetched\n",
      "\n",
      "Video ID: BGodFlkmmm8\n",
      "  Uploaded at: Unknown\n",
      "  Caption Preview: Not Fetched\n",
      "\n",
      "Video ID: GGNcuhKh6PY\n",
      "  Uploaded at: Unknown\n",
      "  Caption Preview: Not Fetched\n",
      "\n",
      "Video ID: kI2PD4cl2Z4\n",
      "  Uploaded at: Unknown\n",
      "  Caption Preview: Not Fetched\n",
      "\n",
      "Video ID: -QzhouA4x-8\n",
      "  Uploaded at: 2017-01-03T01:17:10Z\n",
      "  Caption Preview: how is it going today loyalist K my name is DNE Wayne Jaz today we have this is a commentary 134 leave it in\n",
      "\n",
      "Video ID: -eaNR0tVZkk\n",
      "  Uploaded at: 2017-02-25T00:00:00Z\n",
      "  Caption Preview: probably one of the hardest guns to use in the game if I do say so definitely not going easy Get Ready Get Ready Get\n",
      "\n",
      "Video ID: 012en7bL0p0\n",
      "  Uploaded at: 2017-01-18T03:28:24Z\n",
      "  Caption Preview: that is awesome what is that nice thank you very much I may have played this game for ages but we're\n",
      "\n",
      "Video ID: 1KpLDC8W5jg\n",
      "  Uploaded at: 2017-03-14T01:00:00Z\n",
      "  Caption Preview: # pray for Alia definitely needs to come into effect me so ladies and gentlemen I hope you're having a fantastic day\n",
      "\n",
      "Video ID: 5J_50xxITno\n",
      "  Uploaded at: 2017-03-06T00:00:00Z\n",
      "  Caption Preview: oh my God this gun is insane like one of the best guns in the game so guys today I have been smashing Call of Duty I\n",
      "\n",
      "Video ID: 6XoC3-aDN4M\n",
      "  Uploaded at: 2017-03-16T01:00:01Z\n",
      "  Caption Preview: U wow UAV wow that was some good range actually great great gun ladies and\n",
      "\n",
      "Video ID: 6nWE6RkeTgI\n",
      "  Uploaded at: 2017-02-07T00:01:42Z\n",
      "  Caption Preview: ladies and gentlemen of almost 30,000 of you guys participating I ran a poll on Twitter asking you which game you want\n",
      "\n",
      "Video ID: 7K5D7j9Eb6Y\n",
      "  Uploaded at: 2017-03-06T23:00:00Z\n",
      "  Caption Preview: I expect almost everyone to be using this weapon if it's the new free DLC gun ladies and gentlemen it it is finally\n",
      "\n",
      "Video ID: 7yFGleDcSEU\n",
      "  Uploaded at: 2017-01-22T01:00:00Z\n",
      "  Caption Preview: number six so about a year ago Alia made a bet with n shot at a Forza tournament that if n shot's video got 100,000 likes\n",
      "\n",
      "Video ID: 8-BrUHxO4UU\n",
      "  Uploaded at: 2017-02-18T00:03:48Z\n",
      "  Caption Preview: C run what's up what's up what's up anyone else we've got our snap wig on oh my go so guys following the trend\n",
      "\n",
      "Video ID: 9QetCNkbfBE\n",
      "  Uploaded at: 2017-03-19T00:00:00Z\n",
      "  Caption Preview: let's go first-ever overwatch game we ever played on find your whiskers I know the ugly side article to the fruits of\n",
      "\n",
      "Video ID: A-FPzmfHee0\n",
      "  Uploaded at: 2017-03-08T22:00:01Z\n",
      "  Caption Preview: [Music] jez I forgot how intense this game got as well freaking awesome so guys I was\n",
      "\n",
      "Video ID: B7X1M94qjrM\n",
      "  Uploaded at: 2017-01-02T00:09:11Z\n",
      "  Caption Preview: so guys if you've been following my Adventures on Black Ops 3 recently you'll know that they added in a few new\n",
      "\n",
      "Video ID: DTya-Jbd6Hg\n",
      "  Uploaded at: 2017-03-12T00:00:02Z\n",
      "  Caption Preview: hello everyone my name is Al and welcome to the video you've all been waiting for I guess welcome everyone this it\n",
      "\n",
      "Video ID: HJ3lMCDyr2o\n",
      "  Uploaded at: 2017-02-19T01:00:00Z\n",
      "  Caption Preview: ladies and gentlemen you probably know what is about to happen we cannot have any more of a throwback Series in this\n",
      "\n",
      "Video ID: HeHC0WlHC_I\n",
      "  Uploaded at: 2017-03-08T00:00:01Z\n",
      "  Caption Preview: so guys the very first modern warfare remaster DLC has just been announced a few of you guys have tweeted at me and I\n",
      "\n",
      "Video ID: IOut7pYgBMA\n",
      "  Uploaded at: 2017-03-09T22:00:01Z\n",
      "  Caption Preview: if I had to go through and give my opinion on which gun I thought was the best which one would I decide welcome\n",
      "\n",
      "Video ID: IwrQFar1B4U\n",
      "  Uploaded at: 2017-01-15T00:05:17Z\n",
      "  Caption Preview: this happens to me all the time and I'm sure you guys can relate especially when you just really should be doing\n",
      "\n",
      "Video ID: KpkQbnCPv_w\n",
      "  Uploaded at: 2017-01-12T22:06:37Z\n",
      "  Caption Preview: so I've just finished reacting to some of my old videos my old cool of Duty videos going down memory lane it was so\n",
      "\n",
      "Video ID: KvwOXyHVLNg\n",
      "  Uploaded at: 2017-01-27T00:05:49Z\n",
      "  Caption Preview: oh God I'm dead I'm dead oh oh my God hey surprise surprise surprise guys\n",
      "\n",
      "Video ID: NBgjMkObPmI\n",
      "  Uploaded at: 2017-02-08T22:18:18Z\n",
      "  Caption Preview: [Music] [Applause] [Music]\n",
      "\n",
      "Video ID: NENvvgcoVRg\n",
      "  Uploaded at: 2017-03-23T00:00:01Z\n",
      "  Caption Preview: [Music] we're going to do it we're going to play all of the maps come on air strike\n",
      "\n",
      "Video ID: OWQz2VjltT0\n",
      "  Uploaded at: 2017-01-19T22:00:02Z\n",
      "  Caption Preview: so I'm sure every single one of you by this point has been click baited into clicking a th000 degree knife video and\n",
      "\n",
      "Video ID: PTGzolusEI8\n",
      "  Uploaded at: 2017-02-07T23:07:41Z\n",
      "  Caption Preview: [Music] w [Music]\n",
      "\n",
      "Video ID: QaDgeSr2RrA\n",
      "  Uploaded at: 2017-03-11T01:00:02Z\n",
      "  Caption Preview: ladies and gentlemen today is a monumentous day a day that I am very excited for because believe it or not\n",
      "\n",
      "Video ID: SkSmtpmkXKU\n",
      "  Uploaded at: 2017-02-05T00:21:55Z\n",
      "  Caption Preview: we have to get at least one kill with these things you know what I mean yeah two will do two will do dude you know\n",
      "\n",
      "Video ID: SvSNshPNQnI\n",
      "  Uploaded at: 2017-02-15T01:00:01Z\n",
      "  Caption Preview: [Music] [Music] even at close range this thing\n",
      "\n",
      "Video ID: T7XFQ6KSMkk\n",
      "  Uploaded at: 2017-02-23T00:00:02Z\n",
      "  Caption Preview: they be infected they have like a million more Health jeez so many infected coming at me from\n",
      "\n",
      "Video ID: UdZSGd35qQY\n",
      "  Uploaded at: 2017-02-12T01:00:01Z\n",
      "  Caption Preview: it looks like the M16 but how powerful is this thing let's have a look oh my this could be potentially ridiculous\n",
      "\n",
      "Video ID: W1wI48Ok2v4\n",
      "  Uploaded at: 2017-03-19T22:00:02Z\n",
      "  Caption Preview: so guys it took me a little bit of hunting but I have found it Advanced Warfare my friends is the first edition\n",
      "\n",
      "Video ID: XmUvlHTX-9w\n",
      "  Uploaded at: 2017-03-17T02:00:02Z\n",
      "  Caption Preview: [Music] Che so out of nowhere the Black Ops 3 gods have dropped us a brand new set of\n",
      "\n",
      "Video ID: Z7Qzke8R1g8\n",
      "  Uploaded at: 2017-03-26T01:00:03Z\n",
      "  Caption Preview: oh you ain't safe F oo jeez so guys cod's a little bit\n",
      "\n",
      "Video ID: e4IzWgWp9cQ\n",
      "  Uploaded at: 2017-03-04T00:00:02Z\n",
      "  Caption Preview: oh what welcome everyone I hope you're having an amazing amazing day CLA and I have actually just returned from La\n",
      "\n",
      "Video ID: f27O3MNjYsA\n",
      "  Uploaded at: 2017-03-25T02:25:49Z\n",
      "  Caption Preview: call of duty 2017 the brand-new code title is look set to be cooled cool of duty World War two around this time\n",
      "\n",
      "Video ID: iNxPdm8lqDE\n",
      "  Uploaded at: 2017-02-10T22:00:02Z\n",
      "  Caption Preview: Call of Duty 2017 what is going to be happening are we going to be going back into space are\n",
      "\n",
      "Video ID: jzorGG5LfSw\n",
      "  Uploaded at: 2017-01-25T00:00:02Z\n",
      "  Caption Preview: aliia what do you honestly think and why do you honestly not play Infinite Warfare It's a question I see all the\n",
      "\n",
      "Video ID: k2S5I7DVnF0\n",
      "  Uploaded at: 2017-01-05T01:27:30Z\n",
      "  Caption Preview: so I think whenever I meet people for the first time the most common thing they ever say is wow you're a lot taller\n",
      "\n",
      "Video ID: kamx-pZKe2w\n",
      "  Uploaded at: 2017-01-10T00:13:31Z\n",
      "  Caption Preview: so since we seem to be in a trend of going back and using weapons I never made videos for there's one which is\n",
      "\n",
      "Video ID: onGICftOxIM\n",
      "  Uploaded at: 2017-01-07T22:06:42Z\n",
      "  Caption Preview: i'm wearing this hat it only means one thing we have to be playing Call of Duty Black Ops 3 and actually a weapon I have\n",
      "\n",
      "Video ID: pp7BccKNJoQ\n",
      "  Uploaded at: 2017-02-03T00:09:42Z\n",
      "  Caption Preview: Captions disabled or unavailable\n",
      "\n",
      "Video ID: u5qkiuy7NdE\n",
      "  Uploaded at: 2017-02-01T01:18:52Z\n",
      "  Caption Preview: no enjoy to make only how the hell are we to judge nice ass right they're going to persecute oh I saw I was good of time\n",
      "\n",
      "Video ID: zyUXJ3Sf1UU\n",
      "  Uploaded at: 2017-02-27T00:00:04Z\n",
      "  Caption Preview: Oh lazy lazy and what just happened is was unbelievable it is not by any stretch of imagination\n",
      "\n",
      "Video ID: B154_5CKYN4\n",
      "  Uploaded at: 2017-01-08T18:00:00Z\n",
      "  Caption Preview: [Music] [Music] so guys welcome back to another\n",
      "\n",
      "Video ID: CQ-VqkKqvnU\n",
      "  Uploaded at: 2017-02-05T18:00:01Z\n",
      "  Caption Preview: [Music] guys welcome back to another challenge alley episode today my friend master\n",
      "\n",
      "Video ID: CjDcpM-G6DM\n",
      "  Uploaded at: 2017-01-03T18:00:00Z\n",
      "  Caption Preview: [Music] welcome everyone back to some more awesome VR action today we are using the\n",
      "\n",
      "Video ID: Ua4mvGz5Gxw\n",
      "  Uploaded at: 2017-01-26T18:00:00Z\n",
      "  Caption Preview: oh my [Music] God so guys apparently someone here in\n",
      "\n",
      "Video ID: WbFvacx5W08\n",
      "  Uploaded at: 2017-01-10T18:00:01Z\n",
      "  Caption Preview: welcome everyone to another tactical Tuesday hopefully you guys are having an amazing day I'm excited because we're\n",
      "\n",
      "Video ID: ezE9ffg13gU\n",
      "  Uploaded at: 2016-12-29T18:00:02Z\n",
      "  Caption Preview: [Music] so guys who wants to see behind the scenes bloopers never seen before clips\n",
      "\n",
      "Video ID: snRnWILLwDI\n",
      "  Uploaded at: 2017-01-05T18:00:05Z\n",
      "  Caption Preview: welcome everyone today I'm playing Titan full - one of the awesome FPS games at launched last year I played it\n",
      "\n",
      "Video ID: xuD1egfYRVM\n",
      "  Uploaded at: 2017-02-09T18:00:06Z\n",
      "  Caption Preview: [Music] welcome everyone to a challenge early episodes today grants plan of six\n",
      "\n",
      "Video ID: ClzENBgP4mk\n",
      "  Uploaded at: Unknown\n",
      "  Caption Preview: Not Fetched\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "import requests\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "\n",
    "file_path = 'extracted_data.csv'\n",
    "\n",
    "video_ids = []\n",
    "channel_ids = []\n",
    "cluster_ids = []\n",
    "\n",
    "# Read only the first 100 video IDs\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        try:\n",
    "            cluster_ids.append(int(row['cluster']))\n",
    "            channel_ids.append(row['from'])\n",
    "\n",
    "            videos = ast.literal_eval(row['videos'])  # Convert string to list\n",
    "            video_ids.extend(videos)\n",
    "            if len(video_ids) >= 100:\n",
    "                video_ids = video_ids[:100]\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipping row due to error: {e}\")\n",
    "\n",
    "# YouTube API\n",
    "API_KEY = 'AIzaSyDO9W2xxpc7ud4W8N9L06n2Mwv5QkMtoFc'\n",
    "url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "\n",
    "def chunkify(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "timestamps = {}\n",
    "captions = {}\n",
    "\n",
    "print(\"📡 Fetching video metadata and captions...\")\n",
    "\n",
    "for chunk_num, chunk in enumerate(chunkify(video_ids, 50)):\n",
    "    print(f\"🔹 Processing chunk {chunk_num + 1}/{(len(video_ids) - 1) // 50 + 1}...\")\n",
    "    ids_str = \",\".join(chunk)\n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"id\": ids_str,\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error fetching video data: {e}\")\n",
    "        continue\n",
    "\n",
    "    for item in data.get(\"items\", []):\n",
    "        video_id = item[\"id\"]\n",
    "        published_at = item[\"snippet\"][\"publishedAt\"]\n",
    "        timestamps[video_id] = published_at\n",
    "\n",
    "        try:\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
    "            preview = \" \".join([line['text'] for line in transcript[:3]])\n",
    "            captions[video_id] = preview\n",
    "        except TranscriptsDisabled:\n",
    "            captions[video_id] = \"Captions disabled or unavailable\"\n",
    "        except NoTranscriptFound:\n",
    "            captions[video_id] = \"Captions not available in English\"\n",
    "        except Exception as e:\n",
    "            captions[video_id] = f\"Error fetching captions: {str(e)}\"\n",
    "\n",
    "# ✅ Print the result for top 100\n",
    "for vid in video_ids:\n",
    "    print(f\"Video ID: {vid}\")\n",
    "    print(f\"  Uploaded at: {timestamps.get(vid, 'Unknown')}\")\n",
    "    print(f\"  Caption Preview: {captions.get(vid, 'Not Fetched')}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdfc6d8b-9a08-4441-b988-b2c60587be90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📡 Fetching video metadata and captions...\n",
      "🔹 Processing chunk 1/20...\n",
      "🔹 Processing chunk 2/20...\n",
      "🔹 Processing chunk 3/20...\n",
      "🔹 Processing chunk 4/20...\n",
      "🔹 Processing chunk 5/20...\n",
      "🔹 Processing chunk 6/20...\n",
      "🔹 Processing chunk 7/20...\n",
      "🔹 Processing chunk 8/20...\n",
      "🔹 Processing chunk 9/20...\n",
      "🔹 Processing chunk 10/20...\n",
      "🔹 Processing chunk 11/20...\n",
      "🔹 Processing chunk 12/20...\n",
      "🔹 Processing chunk 13/20...\n",
      "🔹 Processing chunk 14/20...\n",
      "🔹 Processing chunk 15/20...\n",
      "🔹 Processing chunk 16/20...\n",
      "🔹 Processing chunk 17/20...\n",
      "🔹 Processing chunk 18/20...\n",
      "🔹 Processing chunk 19/20...\n",
      "🔹 Processing chunk 20/20...\n",
      "✅ Output for top 1000 videos saved to video_metadata_1000.txt\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "import requests\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "\n",
    "file_path = 'extracted_data.csv'\n",
    "\n",
    "video_ids = []\n",
    "channel_ids = []\n",
    "cluster_ids = []\n",
    "\n",
    "# ✅ Read up to 1000 video IDs\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        try:\n",
    "            cluster_ids.append(int(row['cluster']))\n",
    "            channel_ids.append(row['from'])\n",
    "\n",
    "            videos = ast.literal_eval(row['videos'])\n",
    "            video_ids.extend(videos)\n",
    "            if len(video_ids) >= 1000:\n",
    "                video_ids = video_ids[:1000]\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipping row due to error: {e}\")\n",
    "\n",
    "# 🔑 YouTube API\n",
    "API_KEY = 'AIzaSyDO9W2xxpc7ud4W8N9L06n2Mwv5QkMtoFc'\n",
    "url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "\n",
    "def chunkify(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "timestamps = {}\n",
    "captions = {}\n",
    "\n",
    "print(\"📡 Fetching video metadata and captions...\")\n",
    "\n",
    "for chunk_num, chunk in enumerate(chunkify(video_ids, 50)):\n",
    "    print(f\"🔹 Processing chunk {chunk_num + 1}/{(len(video_ids) - 1) // 50 + 1}...\")\n",
    "    ids_str = \",\".join(chunk)\n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"id\": ids_str,\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error fetching video data: {e}\")\n",
    "        continue\n",
    "\n",
    "    for item in data.get(\"items\", []):\n",
    "        video_id = item[\"id\"]\n",
    "        published_at = item[\"snippet\"][\"publishedAt\"]\n",
    "        timestamps[video_id] = published_at\n",
    "\n",
    "        try:\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
    "            preview = \" \".join([line['text'] for line in transcript[:3]])\n",
    "            captions[video_id] = preview\n",
    "        except TranscriptsDisabled:\n",
    "            captions[video_id] = \"Captions disabled or unavailable\"\n",
    "        except NoTranscriptFound:\n",
    "            captions[video_id] = \"Captions not available in English\"\n",
    "        except Exception as e:\n",
    "            captions[video_id] = f\"Error fetching captions: {str(e)}\"\n",
    "\n",
    "# ✅ Save to file instead of printing\n",
    "with open(\"video_metadata_1000.txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for vid in video_ids:\n",
    "        out_file.write(f\"Video ID: {vid}\\n\")\n",
    "        out_file.write(f\"  Uploaded at: {timestamps.get(vid, 'Unknown')}\\n\")\n",
    "        out_file.write(f\"  Caption Preview: {captions.get(vid, 'Not Fetched')}\\n\\n\")\n",
    "\n",
    "print(\"✅ Output for top 1000 videos saved to video_metadata_1000.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2aa0165-7e04-4c56-9b7f-0102067fb99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📡 Fetching video metadata and captions (1000 to 2000)...\n",
      "🔹 Processing chunk 1/20...\n",
      "🔹 Processing chunk 2/20...\n",
      "🔹 Processing chunk 3/20...\n",
      "🔹 Processing chunk 4/20...\n",
      "🔹 Processing chunk 5/20...\n",
      "🔹 Processing chunk 6/20...\n",
      "🔹 Processing chunk 7/20...\n",
      "🔹 Processing chunk 8/20...\n",
      "🔹 Processing chunk 9/20...\n",
      "🔹 Processing chunk 10/20...\n",
      "🔹 Processing chunk 11/20...\n",
      "🔹 Processing chunk 12/20...\n",
      "🔹 Processing chunk 13/20...\n",
      "🔹 Processing chunk 14/20...\n",
      "🔹 Processing chunk 15/20...\n",
      "🔹 Processing chunk 16/20...\n",
      "🔹 Processing chunk 17/20...\n",
      "🔹 Processing chunk 18/20...\n",
      "🔹 Processing chunk 19/20...\n",
      "🔹 Processing chunk 20/20...\n",
      "✅ Output for video IDs 1000 to 2000 saved to video_metadata_1000_to_2000.txt\n"
     ]
    }
   ],
   "source": [
    "#from 1000 to 2000 video id \n",
    "import csv\n",
    "import ast\n",
    "import requests\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "\n",
    "file_path = 'extracted_data.csv'\n",
    "\n",
    "video_ids = []\n",
    "channel_ids = []\n",
    "cluster_ids = []\n",
    "\n",
    "# ✅ Skip first 1000 and collect next 1000 video IDs\n",
    "skipped = 0\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        try:\n",
    "            cluster_ids.append(int(row['cluster']))\n",
    "            channel_ids.append(row['from'])\n",
    "\n",
    "            videos = ast.literal_eval(row['videos'])\n",
    "            if skipped + len(videos) <= 1000:\n",
    "                skipped += len(videos)\n",
    "                continue  # skip\n",
    "            elif skipped < 1000:\n",
    "                # Partially skip and take the remaining\n",
    "                take = 1000 - skipped\n",
    "                video_ids.extend(videos[take:])\n",
    "                skipped = 1000\n",
    "            else:\n",
    "                video_ids.extend(videos)\n",
    "\n",
    "            if len(video_ids) >= 1000:\n",
    "                video_ids = video_ids[:1000]\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipping row due to error: {e}\")\n",
    "\n",
    "# 🔑 YouTube API\n",
    "API_KEY = 'AIzaSyDO9W2xxpc7ud4W8N9L06n2Mwv5QkMtoFc'\n",
    "url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "\n",
    "def chunkify(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "timestamps = {}\n",
    "captions = {}\n",
    "\n",
    "print(\"📡 Fetching video metadata and captions (1000 to 2000)...\")\n",
    "\n",
    "for chunk_num, chunk in enumerate(chunkify(video_ids, 50)):\n",
    "    print(f\"🔹 Processing chunk {chunk_num + 1}/{(len(video_ids) - 1) // 50 + 1}...\")\n",
    "    ids_str = \",\".join(chunk)\n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"id\": ids_str,\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error fetching video data: {e}\")\n",
    "        continue\n",
    "\n",
    "    for item in data.get(\"items\", []):\n",
    "        video_id = item[\"id\"]\n",
    "        published_at = item[\"snippet\"][\"publishedAt\"]\n",
    "        timestamps[video_id] = published_at\n",
    "\n",
    "        try:\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
    "            preview = \" \".join([line['text'] for line in transcript[:3]])\n",
    "            captions[video_id] = preview\n",
    "        except TranscriptsDisabled:\n",
    "            captions[video_id] = \"Captions disabled or unavailable\"\n",
    "        except NoTranscriptFound:\n",
    "            captions[video_id] = \"Captions not available in English\"\n",
    "        except Exception as e:\n",
    "            captions[video_id] = f\"Error fetching captions: {str(e)}\"\n",
    "\n",
    "# ✅ Save output to a new file for 1000-2000\n",
    "with open(\"video_metadata_1000_to_2000.txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for vid in video_ids:\n",
    "        out_file.write(f\"Video ID: {vid}\\n\")\n",
    "        out_file.write(f\"  Uploaded at: {timestamps.get(vid, 'Unknown')}\\n\")\n",
    "        out_file.write(f\"  Caption Preview: {captions.get(vid, 'Not Fetched')}\\n\\n\")\n",
    "\n",
    "print(\"✅ Output for video IDs 1000 to 2000 saved to video_metadata_1000_to_2000.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ac6e8a0-a863-4b45-82e0-7755d9fc4358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Total video IDs collected after skipping 2000: 5614\n",
      "📡 Fetching video metadata and captions (2000 onwards)...\n",
      "🔹 Processing chunk 1/113...\n",
      "🔹 Processing chunk 2/113...\n",
      "🔹 Processing chunk 3/113...\n",
      "🔹 Processing chunk 4/113...\n",
      "🔹 Processing chunk 5/113...\n",
      "🔹 Processing chunk 6/113...\n",
      "🔹 Processing chunk 7/113...\n",
      "🔹 Processing chunk 8/113...\n",
      "🔹 Processing chunk 9/113...\n",
      "🔹 Processing chunk 10/113...\n",
      "🔹 Processing chunk 11/113...\n",
      "🔹 Processing chunk 12/113...\n",
      "🔹 Processing chunk 13/113...\n",
      "🔹 Processing chunk 14/113...\n",
      "🔹 Processing chunk 15/113...\n",
      "🔹 Processing chunk 16/113...\n",
      "🔹 Processing chunk 17/113...\n",
      "🔹 Processing chunk 18/113...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     68\u001b[39m timestamps[video_id] = published_at\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     transcript = \u001b[43mYouTubeTranscriptApi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_transcript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43men\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     preview = \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join([line[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m transcript[:\u001b[32m3\u001b[39m]])\n\u001b[32m     73\u001b[39m     captions[video_id] = preview\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/youtube_transcript_api/_api.py:306\u001b[39m, in \u001b[36mYouTubeTranscriptApi.get_transcript\u001b[39m\u001b[34m(cls, video_id, languages, proxies, cookies, preserve_formatting)\u001b[39m\n\u001b[32m    298\u001b[39m warnings.warn(\n\u001b[32m    299\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m`get_transcript` is deprecated and will be removed in a future version. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    300\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mUse the `fetch` method instead!\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    301\u001b[39m     \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    302\u001b[39m )\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(video_id, \u001b[38;5;28mstr\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33m`video_id` must be a string\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlist_transcripts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcookies\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m     .find_transcript(languages)\n\u001b[32m    308\u001b[39m     .fetch(preserve_formatting=preserve_formatting)\n\u001b[32m    309\u001b[39m     .to_raw_data()\n\u001b[32m    310\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/youtube_transcript_api/_api.py:207\u001b[39m, in \u001b[36mYouTubeTranscriptApi.list_transcripts\u001b[39m\u001b[34m(cls, video_id, proxies, cookies)\u001b[39m\n\u001b[32m    199\u001b[39m         proxy_config = GenericProxyConfig(\n\u001b[32m    200\u001b[39m             http_url=proxies.get(\u001b[33m\"\u001b[39m\u001b[33mhttp\u001b[39m\u001b[33m\"\u001b[39m), https_url=proxies.get(\u001b[33m\"\u001b[39m\u001b[33mhttps\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    201\u001b[39m         )\n\u001b[32m    203\u001b[39m ytt_api = YouTubeTranscriptApi(\n\u001b[32m    204\u001b[39m     proxy_config=proxy_config,\n\u001b[32m    205\u001b[39m     cookie_path=Path(cookies) \u001b[38;5;28;01mif\u001b[39;00m cookies \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    206\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mytt_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/youtube_transcript_api/_api.py:135\u001b[39m, in \u001b[36mYouTubeTranscriptApi.list\u001b[39m\u001b[34m(self, video_id)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mlist\u001b[39m(\n\u001b[32m     85\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     86\u001b[39m     video_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m     87\u001b[39m ) -> TranscriptList:\n\u001b[32m     88\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[33;03m    Retrieves the list of transcripts which are available for a given video. It\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[33;03m    returns a `TranscriptList` object which is iterable and provides methods to\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    133\u001b[39m \u001b[33;03m        Make sure that this is the actual ID, NOT the full URL to the video!\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/youtube_transcript_api/_transcripts.py:352\u001b[39m, in \u001b[36mTranscriptListFetcher.fetch\u001b[39m\u001b[34m(self, video_id)\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, video_id: \u001b[38;5;28mstr\u001b[39m) -> TranscriptList:\n\u001b[32m    349\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m TranscriptList.build(\n\u001b[32m    350\u001b[39m         \u001b[38;5;28mself\u001b[39m._http_client,\n\u001b[32m    351\u001b[39m         video_id,\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_captions_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    353\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/youtube_transcript_api/_transcripts.py:358\u001b[39m, in \u001b[36mTranscriptListFetcher._fetch_captions_json\u001b[39m\u001b[34m(self, video_id, try_number)\u001b[39m\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_fetch_captions_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, video_id: \u001b[38;5;28mstr\u001b[39m, try_number: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m) -> Dict:\n\u001b[32m    356\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    357\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._extract_captions_json(\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_video_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m, video_id\n\u001b[32m    359\u001b[39m         )\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m RequestBlocked \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m    361\u001b[39m         retries = (\n\u001b[32m    362\u001b[39m             \u001b[32m0\u001b[39m\n\u001b[32m    363\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._proxy_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    364\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._proxy_config.retries_when_blocked\n\u001b[32m    365\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/youtube_transcript_api/_transcripts.py:428\u001b[39m, in \u001b[36mTranscriptListFetcher._fetch_video_html\u001b[39m\u001b[34m(self, video_id)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_fetch_video_html\u001b[39m(\u001b[38;5;28mself\u001b[39m, video_id: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m     html = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    429\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33maction=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://consent.youtube.com/s\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m html:\n\u001b[32m    430\u001b[39m         \u001b[38;5;28mself\u001b[39m._create_consent_cookie(html, video_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/youtube_transcript_api/_transcripts.py:437\u001b[39m, in \u001b[36mTranscriptListFetcher._fetch_html\u001b[39m\u001b[34m(self, video_id)\u001b[39m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_fetch_html\u001b[39m(\u001b[38;5;28mself\u001b[39m, video_id: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_http_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWATCH_URL\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m unescape(_raise_http_errors(response, video_id).text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/requests/sessions.py:602\u001b[39m, in \u001b[36mSession.get\u001b[39m\u001b[34m(self, url, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[32m    595\u001b[39m \n\u001b[32m    596\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[33;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m         \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.is_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.proxy_is_verified:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/urllib3/connection.py:741\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    738\u001b[39m     \u001b[38;5;66;03m# Remove trailing '.' from fqdn hostnames to allow certificate validation\u001b[39;00m\n\u001b[32m    739\u001b[39m     server_hostname_rm_dot = server_hostname.rstrip(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m     sock_and_verified = \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    743\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    744\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    745\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    749\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    755\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    757\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    759\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = sock_and_verified.socket\n\u001b[32m    761\u001b[39m \u001b[38;5;66;03m# If an error occurs during connection/handshake we may need to release\u001b[39;00m\n\u001b[32m    762\u001b[39m \u001b[38;5;66;03m# our lock so another connection can probe the origin.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/urllib3/connection.py:920\u001b[39m, in \u001b[36m_ssl_wrap_socket_and_match_hostname\u001b[39m\u001b[34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[39m\n\u001b[32m    917\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[32m    918\u001b[39m         server_hostname = normalized\n\u001b[32m--> \u001b[39m\u001b[32m920\u001b[39m ssl_sock = \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/urllib3/util/ssl_.py:460\u001b[39m, in \u001b[36mssl_wrap_socket\u001b[39m\u001b[34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[39m\n\u001b[32m    456\u001b[39m         context.load_cert_chain(certfile, keyfile, key_password)\n\u001b[32m    458\u001b[39m context.set_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m ssl_sock = \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/urllib3/util/ssl_.py:504\u001b[39m, in \u001b[36m_ssl_wrap_socket_impl\u001b[39m\u001b[34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[39m\n\u001b[32m    501\u001b[39m     SSLTransport._validate_ssl_context_for_tls_in_tls(ssl_context)\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/ssl.py:455\u001b[39m, in \u001b[36mSSLContext.wrap_socket\u001b[39m\u001b[34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    450\u001b[39m                 do_handshake_on_connect=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    451\u001b[39m                 suppress_ragged_eofs=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    452\u001b[39m                 server_hostname=\u001b[38;5;28;01mNone\u001b[39;00m, session=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msslsocket_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/ssl.py:1041\u001b[39m, in \u001b[36mSSLSocket._create\u001b[39m\u001b[34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[39m\n\u001b[32m   1038\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m timeout == \u001b[32m0.0\u001b[39m:\n\u001b[32m   1039\u001b[39m                 \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[32m   1040\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1041\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1043\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/ssl.py:1319\u001b[39m, in \u001b[36mSSLSocket.do_handshake\u001b[39m\u001b[34m(self, block)\u001b[39m\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout == \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[32m   1318\u001b[39m         \u001b[38;5;28mself\u001b[39m.settimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1321\u001b[39m     \u001b[38;5;28mself\u001b[39m.settimeout(timeout)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "import requests\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "\n",
    "file_path = 'extracted_data.csv'\n",
    "\n",
    "video_ids = []\n",
    "channel_ids = []\n",
    "cluster_ids = []\n",
    "\n",
    "# ✅ Skip first 2000 video IDs and collect the rest\n",
    "skipped = 0\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        try:\n",
    "            cluster_ids.append(int(row['cluster']))\n",
    "            channel_ids.append(row['from'])\n",
    "\n",
    "            videos = ast.literal_eval(row['videos'])\n",
    "            if skipped + len(videos) <= 2000:\n",
    "                skipped += len(videos)\n",
    "                continue  # skip\n",
    "            elif skipped < 2000:\n",
    "                take = 2000 - skipped\n",
    "                video_ids.extend(videos[take:])\n",
    "                skipped = 2000\n",
    "            else:\n",
    "                video_ids.extend(videos)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipping row due to error: {e}\")\n",
    "\n",
    "print(f\"📦 Total video IDs collected after skipping 2000: {len(video_ids)}\")\n",
    "\n",
    "# 🔑 YouTube API\n",
    "API_KEY = 'AIzaSyDO9W2xxpc7ud4W8N9L06n2Mwv5QkMtoFc'\n",
    "url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "\n",
    "def chunkify(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "timestamps = {}\n",
    "captions = {}\n",
    "\n",
    "print(\"📡 Fetching video metadata and captions (2000 onwards)...\")\n",
    "\n",
    "for chunk_num, chunk in enumerate(chunkify(video_ids, 50)):\n",
    "    print(f\"🔹 Processing chunk {chunk_num + 1}/{(len(video_ids) - 1) // 50 + 1}...\")\n",
    "    ids_str = \",\".join(chunk)\n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"id\": ids_str,\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error fetching video data: {e}\")\n",
    "        continue\n",
    "\n",
    "    for item in data.get(\"items\", []):\n",
    "        video_id = item[\"id\"]\n",
    "        published_at = item[\"snippet\"][\"publishedAt\"]\n",
    "        timestamps[video_id] = published_at\n",
    "\n",
    "        try:\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
    "            preview = \" \".join([line['text'] for line in transcript[:3]])\n",
    "            captions[video_id] = preview\n",
    "        except TranscriptsDisabled:\n",
    "            captions[video_id] = \"Captions disabled or unavailable\"\n",
    "        except NoTranscriptFound:\n",
    "            captions[video_id] = \"Captions not available in English\"\n",
    "        except Exception as e:\n",
    "            captions[video_id] = f\"Error fetching captions: {str(e)}\"\n",
    "\n",
    "# ✅ Save output to a new file for 2000 onward\n",
    "with open(\"video_metadata_2000_onwards.txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for vid in video_ids:\n",
    "        out_file.write(f\"Video ID: {vid}\\n\")\n",
    "        out_file.write(f\"  Uploaded at: {timestamps.get(vid, 'Unknown')}\\n\")\n",
    "        out_file.write(f\"  Caption Preview: {captions.get(vid, 'Not Fetched')}\\n\\n\")\n",
    "\n",
    "print(\"✅ Output for video IDs from 2000 onwards saved to video_metadata_2000_onwards.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d656cf64-a291-40d0-aada-eb64cc314847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Total video IDs collected from first 1000 rows: 2147\n",
      "📡 Fetching video metadata and captions...\n",
      "🔹 Processing chunk 1/43...\n",
      "🔹 Processing chunk 2/43...\n",
      "🔹 Processing chunk 3/43...\n",
      "🔹 Processing chunk 4/43...\n",
      "🔹 Processing chunk 5/43...\n",
      "🔹 Processing chunk 6/43...\n",
      "🔹 Processing chunk 7/43...\n",
      "🔹 Processing chunk 8/43...\n",
      "🔹 Processing chunk 9/43...\n",
      "🔹 Processing chunk 10/43...\n",
      "🔹 Processing chunk 11/43...\n",
      "🔹 Processing chunk 12/43...\n",
      "🔹 Processing chunk 13/43...\n",
      "🔹 Processing chunk 14/43...\n",
      "🔹 Processing chunk 15/43...\n",
      "🔹 Processing chunk 16/43...\n",
      "🔹 Processing chunk 17/43...\n",
      "🔹 Processing chunk 18/43...\n",
      "🔹 Processing chunk 19/43...\n",
      "🔹 Processing chunk 20/43...\n",
      "🔹 Processing chunk 21/43...\n",
      "🔹 Processing chunk 22/43...\n",
      "🔹 Processing chunk 23/43...\n",
      "🔹 Processing chunk 24/43...\n",
      "🔹 Processing chunk 25/43...\n",
      "🔹 Processing chunk 26/43...\n",
      "🔹 Processing chunk 27/43...\n",
      "🔹 Processing chunk 28/43...\n",
      "🔹 Processing chunk 29/43...\n",
      "🔹 Processing chunk 30/43...\n",
      "🔹 Processing chunk 31/43...\n",
      "🔹 Processing chunk 32/43...\n",
      "🔹 Processing chunk 33/43...\n",
      "🔹 Processing chunk 34/43...\n",
      "🔹 Processing chunk 35/43...\n",
      "🔹 Processing chunk 36/43...\n",
      "🔹 Processing chunk 37/43...\n",
      "🔹 Processing chunk 38/43...\n",
      "🔹 Processing chunk 39/43...\n",
      "🔹 Processing chunk 40/43...\n",
      "🔹 Processing chunk 41/43...\n",
      "🔹 Processing chunk 42/43...\n",
      "🔹 Processing chunk 43/43...\n",
      "✅ Output for rows 1 to 1000 saved to video_metadata_rows_1_to_1000.txt\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "import requests\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "\n",
    "file_path = 'extracted_data.csv'\n",
    "\n",
    "video_ids = []\n",
    "channel_ids = []\n",
    "cluster_ids = []\n",
    "\n",
    "# ✅ Read exactly first 1000 rows\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for i, row in enumerate(reader):\n",
    "        if i >= 1000:\n",
    "            break\n",
    "        try:\n",
    "            cluster_ids.append(int(row['cluster']))\n",
    "            channel_ids.append(row['from'])\n",
    "\n",
    "            videos = ast.literal_eval(row['videos'])\n",
    "            video_ids.extend(videos)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipping row {i} due to error: {e}\")\n",
    "\n",
    "print(f\"📊 Total video IDs collected from first 1000 rows: {len(video_ids)}\")\n",
    "\n",
    "# 🔑 YouTube API setup\n",
    "API_KEY = 'AIzaSyDO9W2xxpc7ud4W8N9L06n2Mwv5QkMtoFc'\n",
    "url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "\n",
    "def chunkify(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "timestamps = {}\n",
    "captions = {}\n",
    "\n",
    "print(\"📡 Fetching video metadata and captions...\")\n",
    "\n",
    "for chunk_num, chunk in enumerate(chunkify(video_ids, 50)):\n",
    "    print(f\"🔹 Processing chunk {chunk_num + 1}/{(len(video_ids) - 1) // 50 + 1}...\")\n",
    "    ids_str = \",\".join(chunk)\n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"id\": ids_str,\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error fetching video data: {e}\")\n",
    "        continue\n",
    "\n",
    "    for item in data.get(\"items\", []):\n",
    "        video_id = item[\"id\"]\n",
    "        published_at = item[\"snippet\"][\"publishedAt\"]\n",
    "        timestamps[video_id] = published_at\n",
    "\n",
    "        try:\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
    "            preview = \" \".join([line['text'] for line in transcript[:3]])\n",
    "            captions[video_id] = preview\n",
    "        except TranscriptsDisabled:\n",
    "            captions[video_id] = \"Captions disabled or unavailable\"\n",
    "        except NoTranscriptFound:\n",
    "            captions[video_id] = \"Captions not available in English\"\n",
    "        except Exception as e:\n",
    "            captions[video_id] = f\"Error fetching captions: {str(e)}\"\n",
    "\n",
    "# ✅ Save to file\n",
    "with open(\"video_metadata_rows_1_to_1000.txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for vid in video_ids:\n",
    "        out_file.write(f\"Video ID: {vid}\\n\")\n",
    "        out_file.write(f\"  Uploaded at: {timestamps.get(vid, 'Unknown')}\\n\")\n",
    "        out_file.write(f\"  Caption Preview: {captions.get(vid, 'Not Fetched')}\\n\\n\")\n",
    "\n",
    "print(\"✅ Output for rows 1 to 1000 saved to video_metadata_rows_1_to_1000.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
